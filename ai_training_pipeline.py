#!/usr/bin/env python3
"""
AI Training Pipeline - Hybrid Gemini + Ollama System
T·ªëi ∆∞u h√≥a model v·ªõi d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao t·ª´ Gemini ƒë·ªÉ training Ollama
"""

from dataclasses import dataclass
from datetime import datetime
import json
from pathlib import Path
import time
from typing import Dict, List, Optional, Tuple

import requests
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from rich.prompt import Prompt
from rich.table import Table

from main import GeminiAI, Knowledge, WorkspaceLoader

console = Console()


@dataclass
class TrainingExample:
    """C·∫•u tr√∫c d·ªØ li·ªáu training example"""

    question: str
    context: str
    gemini_answer: str
    confidence: float
    timestamp: datetime
    quality_score: float = 0.0
    metadata: Optional[Dict] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class ModelMetrics:
    """Metrics ƒë√°nh gi√° model performance"""

    accuracy: float
    confidence_avg: float
    response_time: float
    training_examples: int
    last_updated: datetime
    model_version: str


class OllamaTrainer:
    """Training v√† fine-tuning Ollama models"""

    def __init__(self, base_url: str = 'http://localhost:11434'):
        self.base_url = base_url
        self.is_available = False
        self.session = requests.Session()
        self._check_availability()

    def _check_availability(self):
        """Ki·ªÉm tra Ollama c√≥ kh·∫£ d·ª•ng kh√¥ng"""
        try:
            response = self.session.get(f'{self.base_url}/api/tags', timeout=5)
            if response.status_code == 200:
                self.is_available = True
                console.print('[green]‚úì Ollama server kh·∫£ d·ª•ng[/green]')
            else:
                console.print('[yellow]‚ö† Ollama server kh√¥ng ph·∫£n h·ªìi[/yellow]')
        except requests.RequestException:
            console.print('[yellow]‚ö† Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Ollama[/yellow]')

    def create_modelfile(
        self, base_model: str, training_data: List[TrainingExample]
    ) -> str:
        """T·∫°o Modelfile cho Ollama t·ª´ training data"""

        # T·∫°o system prompt t·ªëi ∆∞u t·ª´ training examples
        high_quality_examples = [ex for ex in training_data if ex.confidence > 0.8]

        system_prompt = f"""B·∫°n l√† AI Interview Expert chuy√™n s√¢u v·ªÅ Hugging Face v√† Machine Learning.

TRAINING DATA INSIGHTS:
- ƒê√£ h·ªçc t·ª´ {len(high_quality_examples)} examples ch·∫•t l∆∞·ª£ng cao
- Average confidence: {sum(ex.confidence for ex in high_quality_examples) / len(high_quality_examples):.2f}
- C·∫≠p nh·∫≠t l·∫ßn cu·ªëi: {datetime.now().strftime('%Y-%m-%d %H:%M')}

QUY T·∫ÆC PH·∫¢N H·ªíI:
1. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát chuy√™n nghi·ªáp v√† ch√≠nh x√°c
2. S·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ tƒÉng t√≠nh sinh ƒë·ªông
3. KH√îNG s·ª≠ d·ª•ng markdown formatting
4. Cung c·∫•p examples c·ª• th·ªÉ v√† practical
5. T·∫≠p trung v√†o Hugging Face ecosystem

CHUY√äN M√îN:
- Hugging Face Hub, Transformers, Datasets, Spaces
- Machine Learning workflows v√† best practices
- Python programming trong ML context
- Real-world applications v√† troubleshooting

H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c v√† h·ªØu √≠ch!"""

        # T·∫°o training examples cho Modelfile
        examples_text = ''
        for ex in high_quality_examples[:20]:  # Limit to top 20 examples
            examples_text += f"""
USER: {ex.question}
ASSISTANT: {ex.gemini_answer}

"""

        modelfile = f"""FROM {base_model}

SYSTEM "{system_prompt}"

# High-quality training examples from Gemini
{examples_text}

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_predict 512
"""

        return modelfile

    def train_model(self, model_name: str, modelfile_content: str) -> bool:
        """Training model v·ªõi Ollama"""
        try:
            # T·∫°o file t·∫°m cho Modelfile
            modelfile_path = Path(f'/tmp/{model_name}_modelfile')
            with open(modelfile_path, 'w', encoding='utf-8') as f:
                f.write(modelfile_content)

            console.print(f'[yellow]üîß ƒêang training model: {model_name}...[/yellow]')

            # G·ªçi Ollama create command
            import subprocess

            result = subprocess.run(
                ['ollama', 'create', model_name, '-f', str(modelfile_path)],
                capture_output=True,
                text=True,
                timeout=300,
            )

            if result.returncode == 0:
                console.print(
                    f'[green]‚úÖ Model {model_name} ƒë∆∞·ª£c training th√†nh c√¥ng![/green]'
                )
                # Cleanup
                modelfile_path.unlink()
                return True
            else:
                console.print(f'[red]‚ùå L·ªói training: {result.stderr}[/red]')
                return False

        except Exception as e:
            console.print(f'[red]‚ùå L·ªói training model: {e}[/red]')
            return False

    def test_model(self, model_name: str, test_questions: List[str]) -> ModelMetrics:
        """Test model performance"""

        if not self.is_available:
            return ModelMetrics(0, 0, 0, 0, datetime.now(), model_name)

        total_time = 0
        successful_tests = 0

        console.print(f'[yellow]üß™ Testing model: {model_name}...[/yellow]')

        for question in test_questions:
            try:
                start_time = time.time()

                response = self.session.post(
                    f'{self.base_url}/api/generate',
                    json={
                        'model': model_name,
                        'prompt': question,
                        'stream': False,
                        'options': {'temperature': 0.7, 'num_predict': 256},
                    },
                    timeout=30,
                )

                response_time = time.time() - start_time
                total_time += response_time

                if response.status_code == 200:
                    result = response.json()
                    if result.get('response'):
                        successful_tests += 1

            except Exception:
                console.print(
                    f'[dim]Test failed for question: {question[:50]}...[/dim]'
                )

        accuracy = successful_tests / len(test_questions) if test_questions else 0
        avg_response_time = total_time / len(test_questions) if test_questions else 0

        return ModelMetrics(
            accuracy=accuracy,
            confidence_avg=0.8,  # Estimated
            response_time=avg_response_time,
            training_examples=0,
            last_updated=datetime.now(),
            model_version=model_name,
        )


class SmartTrainingPipeline:
    """Pipeline th√¥ng minh k·∫øt h·ª£p Gemini + Ollama"""

    def __init__(self):
        self.gemini_ai = GeminiAI()
        self.ollama_trainer = OllamaTrainer()
        self.workspace_loader = WorkspaceLoader()
        self.training_data: List[TrainingExample] = []
        self.training_history = []

    def generate_training_data(
        self, knowledge_base: List[Knowledge], num_examples: int = 50
    ) -> List[TrainingExample]:
        """T·∫°o training data ch·∫•t l∆∞·ª£ng cao t·ª´ Gemini"""

        console.print(
            f'[bold blue]üöÄ Generating {num_examples} training examples with Gemini...[/bold blue]'
        )

        if not self.gemini_ai.is_available:
            console.print('[red]‚ùå Gemini AI kh√¥ng kh·∫£ d·ª•ng![/red]')
            return []

        training_examples = []

        # T·∫°o c√°c c√¢u h·ªèi t·ª´ knowledge base
        questions = self._generate_questions_from_knowledge(
            knowledge_base, num_examples
        )

        with Progress(
            SpinnerColumn(),
            TextColumn('[progress.description]{task.description}'),
            BarColumn(),
            console=console,
        ) as progress:
            task = progress.add_task(
                'Generating training data...', total=len(questions)
            )

            for i, (question, context) in enumerate(questions):
                try:
                    # Sinh answer v·ªõi Gemini
                    progress.update(
                        task,
                        description=f'Processing question {i + 1}/{len(questions)}',
                    )

                    ai_response = self.gemini_ai.generate_response(question, context)

                    if ai_response.confidence > 0.6:  # Ch·ªâ l·∫•y responses ch·∫•t l∆∞·ª£ng cao
                        example = TrainingExample(
                            question=question,
                            context=context,
                            gemini_answer=ai_response.content,
                            confidence=ai_response.confidence,
                            timestamp=datetime.now(),
                            metadata={
                                'source': 'gemini_generation',
                                'thinking_process': ai_response.thinking_process,
                            },
                        )

                        # ƒê√°nh gi√° quality score
                        example.quality_score = self._calculate_quality_score(example)
                        training_examples.append(example)

                    progress.advance(task)

                    # Rate limiting
                    time.sleep(0.5)

                except Exception as e:
                    console.print(f'[dim]Skipped question due to error: {e}[/dim]')
                    progress.advance(task)

        # S·∫Øp x·∫øp theo quality score
        training_examples.sort(key=lambda x: x.quality_score, reverse=True)

        console.print(
            f'[green]‚úÖ Generated {len(training_examples)} high-quality examples[/green]'
        )
        console.print(
            f'[yellow]Average confidence: {sum(ex.confidence for ex in training_examples) / len(training_examples):.2f}[/yellow]'
        )

        return training_examples

    def _generate_questions_from_knowledge(
        self, knowledge_base: List[Knowledge], num_questions: int
    ) -> List[Tuple[str, str]]:
        """T·∫°o questions t·ª´ knowledge base"""

        question_templates = [
            '{topic} l√† g√¨ v√† t·∫°i sao n√≥ quan tr·ªçng?',
            'C√°ch s·ª≠ d·ª•ng {topic} trong th·ª±c t·∫ø?',
            'So s√°nh {topic} v·ªõi c√°c alternatives kh√°c?',
            '∆Øu ƒëi·ªÉm v√† nh∆∞·ª£c ƒëi·ªÉm c·ªßa {topic}?',
            'Best practices khi l√†m vi·ªác v·ªõi {topic}?',
            'Troubleshooting common issues v·ªõi {topic}?',
            'V√≠ d·ª• c·ª• th·ªÉ v·ªÅ {topic} trong project th·ª±c t·∫ø?',
            'C√°ch optimize performance v·ªõi {topic}?',
        ]

        questions = []
        topics_used = set()

        for knowledge in knowledge_base:
            # Extract topics t·ª´ knowledge
            topics = knowledge.keywords[:5]  # Top 5 keywords

            for topic in topics:
                if len(questions) >= num_questions:
                    break

                if topic not in topics_used and len(topic) > 3:
                    template = question_templates[
                        len(questions) % len(question_templates)
                    ]
                    question = template.format(topic=topic)

                    # Context t·ª´ knowledge
                    context = f'## {knowledge.title}\n{knowledge.content[:1000]}...'

                    questions.append((question, context))
                    topics_used.add(topic)

            if len(questions) >= num_questions:
                break

        return questions

    def _calculate_quality_score(self, example: TrainingExample) -> float:
        """T√≠nh quality score cho training example"""
        score = example.confidence  # Base t·ª´ confidence

        # Bonus cho response length (not too short, not too long)
        response_length = len(example.gemini_answer)
        if 100 <= response_length <= 800:
            score += 0.1

        # Bonus cho presence c·ªßa examples ho·∫∑c code
        if any(
            keyword in example.gemini_answer.lower()
            for keyword in ['v√≠ d·ª•', 'example', 'code', 'python', '```']
        ):
            score += 0.1

        # Bonus cho structured content
        if '‚Ä¢' in example.gemini_answer or example.gemini_answer.count('\n') > 2:
            score += 0.05

        # Bonus cho technical terms
        if any(
            term in example.gemini_answer.lower()
            for term in ['hugging face', 'model', 'api', 'dataset', 'pipeline']
        ):
            score += 0.05

        return min(1.0, score)

    def create_optimized_model(
        self,
        base_model: str = 'llama3:8b',
        model_name: str = None,  # type: ignore
    ) -> bool:
        """T·∫°o model t·ªëi ∆∞u t·ª´ training data"""

        if not model_name:
            model_name = f'hf-expert-{datetime.now().strftime("%Y%m%d")}'

        if not self.training_data:
            console.print('[red]‚ùå Kh√¥ng c√≥ training data! H√£y generate tr∆∞·ªõc.[/red]')
            return False

        if not self.ollama_trainer.is_available:
            console.print('[red]‚ùå Ollama kh√¥ng kh·∫£ d·ª•ng![/red]')
            return False

        console.print(
            f'[bold blue]üîß Creating optimized model: {model_name}[/bold blue]'
        )

        # T·∫°o Modelfile
        modelfile = self.ollama_trainer.create_modelfile(base_model, self.training_data)

        # Training model
        success = self.ollama_trainer.train_model(model_name, modelfile)

        if success:
            # Test model
            test_questions = [ex.question for ex in self.training_data[:10]]
            metrics = self.ollama_trainer.test_model(model_name, test_questions)

            self._display_model_metrics(model_name, metrics)

            # Save training history
            self.training_history.append(
                {
                    'model_name': model_name,
                    'base_model': base_model,
                    'training_examples': len(self.training_data),
                    'timestamp': datetime.now(),
                    'metrics': metrics,
                }
            )

            return True

        return False

    def _display_model_metrics(self, model_name: str, metrics: ModelMetrics):
        """Hi·ªÉn th·ªã metrics c·ªßa model"""

        table = Table(
            title=f'üìä Model Performance: {model_name}', title_style='bold green'
        )
        table.add_column('Metric', style='cyan')
        table.add_column('Value', style='white', justify='right')

        table.add_row('Accuracy', f'{metrics.accuracy:.2%}')
        table.add_row('Avg Response Time', f'{metrics.response_time:.2f}s')
        table.add_row('Training Examples', str(metrics.training_examples))
        table.add_row('Last Updated', metrics.last_updated.strftime('%Y-%m-%d %H:%M'))

        console.print(table)

    def continuous_improvement(self, workspace_path: str, check_interval: int = 3600):
        """Continuous learning v√† improvement"""

        console.print(
            '[bold blue]üîÑ Starting continuous improvement mode...[/bold blue]'
        )
        console.print(f'[yellow]Check interval: {check_interval} seconds[/yellow]')

        last_update = datetime.now()

        while True:
            try:
                console.print(
                    f'\n[dim]{datetime.now().strftime("%H:%M:%S")} - Checking for updates...[/dim]'
                )

                # Load workspace ƒë·ªÉ check updates
                knowledge_base = self.workspace_loader.load_workspace_content(
                    workspace_path
                )

                # Check if c√≥ content m·ªõi
                if self._has_new_content(knowledge_base, last_update):
                    console.print(
                        '[yellow]üÜï Detected new content, generating training data...[/yellow]'
                    )

                    # Generate new training examples
                    new_examples = self.generate_training_data(knowledge_base, 20)

                    if new_examples:
                        # Add to existing training data
                        self.training_data.extend(new_examples)

                        # Keep only best examples (limit memory)
                        self.training_data.sort(
                            key=lambda x: x.quality_score, reverse=True
                        )
                        self.training_data = self.training_data[:200]

                        # Create updated model
                        model_name = f'hf-expert-v{len(self.training_history) + 1}'
                        if self.create_optimized_model(model_name=model_name):
                            console.print(
                                f'[green]‚úÖ Updated model: {model_name}[/green]'
                            )

                        last_update = datetime.now()

                # Sleep until next check
                time.sleep(check_interval)

            except KeyboardInterrupt:
                console.print('\n[yellow]Stopping continuous improvement...[/yellow]')
                break
            except Exception as e:
                console.print(f'[red]Error in continuous improvement: {e}[/red]')
                time.sleep(60)  # Wait 1 minute before retry

    def _has_new_content(
        self, knowledge_base: List[Knowledge], last_update: datetime
    ) -> bool:
        """Check xem c√≥ content m·ªõi kh√¥ng"""
        # Simple implementation - check if files are newer
        for knowledge in knowledge_base:
            try:
                file_mtime = Path(knowledge.source_file).stat().st_mtime
                file_datetime = datetime.fromtimestamp(file_mtime)
                if file_datetime > last_update:
                    return True
            except:  # noqa: E722
                continue
        return False

    def save_training_data(self, filepath: str):
        """Save training data to file"""
        data = {
            'training_examples': [
                {
                    'question': ex.question,
                    'context': ex.context,
                    'gemini_answer': ex.gemini_answer,
                    'confidence': ex.confidence,
                    'quality_score': ex.quality_score,
                    'timestamp': ex.timestamp.isoformat(),
                    'metadata': ex.metadata,
                }
                for ex in self.training_data
            ],
            'training_history': self.training_history,
            'created': datetime.now().isoformat(),
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        console.print(f'[green]üíæ Saved training data to: {filepath}[/green]')

    def load_training_data(self, filepath: str):
        """Load training data from file"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            self.training_data = []
            for ex_data in data['training_examples']:
                example = TrainingExample(
                    question=ex_data['question'],
                    context=ex_data['context'],
                    gemini_answer=ex_data['gemini_answer'],
                    confidence=ex_data['confidence'],
                    timestamp=datetime.fromisoformat(ex_data['timestamp']),
                    quality_score=ex_data.get('quality_score', 0.0),
                    metadata=ex_data.get('metadata', {}),
                )
                self.training_data.append(example)

            self.training_history = data.get('training_history', [])

            console.print(
                f'[green]üì• Loaded {len(self.training_data)} training examples[/green]'
            )

        except Exception as e:
            console.print(f'[red]‚ùå Error loading training data: {e}[/red]')


def main():
    """Main demo function"""
    console.print(
        Panel.fit(
            """[bold blue]üöÄ AI Training Pipeline - Hybrid Gemini + Ollama[/bold blue]
            [green]‚ú® T√≠nh nƒÉng:[/green]
            ‚Ä¢ [bold]Smart Data Generation[/bold] - T·∫°o training data t·ª´ Gemini
            ‚Ä¢ [bold]Ollama Fine-tuning[/bold] - Training local model optimized
            ‚Ä¢ [bold]Continuous Learning[/bold] - T·ª± ƒë·ªông c·∫≠p nh·∫≠t v·ªõi data m·ªõi
            ‚Ä¢ [bold]Quality Scoring[/bold] - ƒê√°nh gi√° v√† filter data ch·∫•t l∆∞·ª£ng cao
            ‚Ä¢ [bold]Performance Metrics[/bold] - Tracking model performance

            [yellow]üéØ Workflow:[/yellow]
            1. Generate high-quality training data v·ªõi Gemini
            2. Fine-tune Ollama model v·ªõi data ƒë√≥
            3. Test v√† evaluate model performance
            4. Continuous improvement v·ªõi data m·ªõi""",
            title='ü§ñ Smart AI Training System',
            border_style='blue',
        )
    )

    pipeline = SmartTrainingPipeline()

    # Check prerequisites
    if not pipeline.gemini_ai.is_available:
        console.print('[red]‚ùå Gemini AI kh√¥ng kh·∫£ d·ª•ng! Check API key.[/red]')
        return

    if not pipeline.ollama_trainer.is_available:
        console.print('[red]‚ùå Ollama kh√¥ng kh·∫£ d·ª•ng! Start ollama serve.[/red]')
        return

    console.print('[green]‚úÖ All systems ready![/green]')

    while True:
        console.print('\n[bold yellow]üìã Choose action:[/bold yellow]')
        console.print('1. üîÑ Generate training data')
        console.print('2. üèóÔ∏è  Create optimized model')
        console.print('3. üß™ Test model performance')
        console.print('4. üîÑ Start continuous improvement')
        console.print('5. üíæ Save/Load training data')
        console.print('6. üìä View training history')
        console.print('0. ‚ùå Exit')

        choice = Prompt.ask(
            'Select option', choices=['0', '1', '2', '3', '4', '5', '6'], default='1'
        )

        if choice == '0':
            console.print('[green]Goodbye! üëã[/green]')
            break
        elif choice == '1':
            # Generate training data
            workspace_path = Prompt.ask('Workspace path', default='.')
            num_examples = int(Prompt.ask('Number of examples', default='50'))

            knowledge_base = pipeline.workspace_loader.load_workspace_content(
                workspace_path
            )
            if knowledge_base:
                pipeline.training_data = pipeline.generate_training_data(
                    knowledge_base, num_examples
                )
            else:
                console.print('[red]‚ùå No knowledge base found![/red]')

        elif choice == '2':
            # Create model
            if not pipeline.training_data:
                console.print('[red]‚ùå No training data! Generate first.[/red]')
                continue

            base_model = Prompt.ask('Base model', default='llama3:8b')
            model_name = Prompt.ask('Model name (optional)', default='')

            pipeline.create_optimized_model(base_model, model_name or None)  # type: ignore

        elif choice == '3':
            # Test model
            model_name = Prompt.ask('Model name to test')
            test_questions = [
                'Hugging Face l√† g√¨?',
                'C√°ch s·ª≠ d·ª•ng transformers library?',
                'Fine-tuning model nh∆∞ th·∫ø n√†o?',
                'So s√°nh BERT v√† GPT?',
                'Hugging Face Hub c√≥ t√≠nh nƒÉng g√¨?',
            ]

            metrics = pipeline.ollama_trainer.test_model(model_name, test_questions)
            pipeline._display_model_metrics(model_name, metrics)

        elif choice == '4':
            # Continuous improvement
            workspace_path = Prompt.ask('Workspace path', default='.')
            interval = int(Prompt.ask('Check interval (seconds)', default='3600'))

            pipeline.continuous_improvement(workspace_path, interval)

        elif choice == '5':
            # Save/Load data
            action = Prompt.ask('Save or Load?', choices=['save', 'load'])
            filepath = Prompt.ask('File path', default='training_data.json')

            if action == 'save':
                pipeline.save_training_data(filepath)
            else:
                pipeline.load_training_data(filepath)

        elif choice == '6':
            # View history
            if pipeline.training_history:
                table = Table(title='üìà Training History', title_style='bold cyan')
                table.add_column('Model', style='white')
                table.add_column('Examples', justify='right', style='magenta')
                table.add_column('Accuracy', justify='right', style='green')
                table.add_column('Date', style='dim')

                for entry in pipeline.training_history:
                    table.add_row(
                        entry['model_name'],
                        str(entry['training_examples']),
                        f'{entry["metrics"].accuracy:.2%}',
                        entry['timestamp'].strftime('%Y-%m-%d %H:%M'),
                    )

                console.print(table)
            else:
                console.print('[yellow]üìã No training history yet[/yellow]')


if __name__ == '__main__':
    main()
