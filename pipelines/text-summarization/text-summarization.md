# Text Summarization vá»›i Hugging Face

## ğŸ“‹ Tá»•ng Quan

Text Summarization lÃ  quÃ¡ trÃ¬nh rÃºt gá»n má»™t Ä‘oáº¡n vÄƒn báº£n dÃ i thÃ nh phiÃªn báº£n ngáº¯n hÆ¡n trong khi váº«n giá»¯ láº¡i cÃ¡c thÃ´ng tin quan trá»ng nháº¥t.

## ğŸ¯ Hai PhÆ°Æ¡ng PhÃ¡p ChÃ­nh

### ğŸ“Š So SÃ¡nh Extractive vs Abstractive

| TiÃªu chÃ­ | Extractive Summarization | Abstractive Summarization |
|----------|-------------------------|---------------------------|
| **Äá»‹nh nghÄ©a** | Chá»n cÃ¡c cÃ¢u quan trá»ng tá»« vÄƒn báº£n gá»‘c | Táº¡o ra vÄƒn báº£n má»›i thá»ƒ hiá»‡n Ã½ chÃ­nh |
| **PhÆ°Æ¡ng phÃ¡p** | TrÃ­ch xuáº¥t cÃ¢u cÃ³ sáºµn | Sinh vÄƒn báº£n má»›i |
| **TÃ i nguyÃªn** | Ãt tÃ i nguyÃªn tÃ­nh toÃ¡n | Nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n |
| **Äá»™ chÃ­nh xÃ¡c** | Cao (khÃ´ng táº¡o thÃ´ng tin má»›i) | Trung bÃ¬nh (cÃ³ thá»ƒ táº¡o thÃ´ng tin sai) |
| **TÃ­nh linh hoáº¡t** | Tháº¥p (giá»¯ nguyÃªn cÃ¢u gá»‘c) | Cao (diá»…n Ä‘áº¡t linh hoáº¡t) |
| **Kháº£ nÄƒng Ä‘á»c** | CÃ³ thá»ƒ kÃ©m máº¡ch láº¡c | Tá»± nhiÃªn vÃ  dá»… Ä‘á»c hÆ¡n |

### ğŸ”„ Workflow Summarization

```mermaid
graph TD
    A[Input Text] --> B{Chá»n PhÆ°Æ¡ng PhÃ¡p}
    B -->|Cáº§n Ä‘á»™ chÃ­nh xÃ¡c cao| C[Extractive]
    B -->|Cáº§n tÃ­nh tá»± nhiÃªn| D[Abstractive]
    
    C --> E[Chá»n Model Extractive]
    D --> F[Chá»n Model Abstractive]
    
    E --> G[Pipeline: summarization]
    F --> G
    
    G --> H[Äáº·t Parameters]
    H --> I[min_length, max_length]
    I --> J[Thá»±c thi]
    J --> K[Summary Output]
    
    style A fill:#e1f5fe
    style K fill:#c8e6c9
    style B fill:#fff3e0
```

## ğŸš€ CÃ¡ch Triá»ƒn Khai

### 1. Extractive Summarization

```python
from transformers import pipeline

# Táº¡o pipeline cho extractive summarization
summarizer = pipeline(
    task="summarization",
    model="sshleifer/distilbart-cnn-6-6"  # Model cho extractive
)

# Input text
text = """
Data Science lÃ  lÄ©nh vá»±c káº¿t há»£p thá»‘ng kÃª, khoa há»c mÃ¡y tÃ­nh 
vÃ  kiáº¿n thá»©c chuyÃªn mÃ´n Ä‘á»ƒ trÃ­ch xuáº¥t insights tá»« dá»¯ liá»‡u...
"""

# Thá»±c hiá»‡n summarization
summary = summarizer(text, max_length=100, min_length=30)
print(summary[0]['summary_text'])
```

### 2. Abstractive Summarization

```python
from transformers import pipeline

# Táº¡o pipeline cho abstractive summarization  
summarizer = pipeline(
    task="summarization",
    model="facebook/bart-large-cnn"  # Model cho abstractive
)

# Thá»±c hiá»‡n summarization vá»›i cÃ¹ng input
summary = summarizer(text, max_length=100, min_length=30)
print(summary[0]['summary_text'])
```

## ğŸ® Use Cases Thá»±c Táº¿

### ğŸ“– Extractive Summarization - Khi NÃ o Sá»­ Dá»¥ng?

| Use Case | MÃ´ táº£ | LÃ½ do chá»n Extractive |
|----------|-------|----------------------|
| **Legal Document Analysis** | PhÃ¢n tÃ­ch há»£p Ä‘á»“ng, vÄƒn báº£n phÃ¡p lÃ½ | Cáº§n Ä‘á»™ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i |
| **Financial Research** | TÃ³m táº¯t bÃ¡o cÃ¡o tÃ i chÃ­nh | KhÃ´ng Ä‘Æ°á»£c táº¡o thÃ´ng tin sai |
| **Academic Papers** | TÃ³m táº¯t nghiÃªn cá»©u khoa há»c | Giá»¯ nguyÃªn thuáº­t ngá»¯ chuyÃªn mÃ´n |
| **Medical Reports** | TÃ³m táº¯t há»“ sÆ¡ bá»‡nh Ã¡n | An toÃ n, khÃ´ng thÃªm thÃ´ng tin |

### âœ¨ Abstractive Summarization - Khi NÃ o Sá»­ Dá»¥ng?

| Use Case | MÃ´ táº£ | LÃ½ do chá»n Abstractive |
|----------|-------|----------------------|
| **News Article Summaries** | TÃ³m táº¯t tin tá»©c | Cáº§n vÄƒn phong tá»± nhiÃªn |
| **Content Recommendations** | MÃ´ táº£ sáº£n pháº©m, ná»™i dung | Cáº§n tÃ­nh háº¥p dáº«n |
| **Blog Post Summaries** | TÃ³m táº¯t bÃ i viáº¿t | Dá»… Ä‘á»c, thu hÃºt |
| **Social Media Posts** | Táº¡o caption, mÃ´ táº£ ngáº¯n | Cáº§n tÃ­nh sÃ¡ng táº¡o |

## âš™ï¸ Tham Sá»‘ Quan Trá»ng

### ğŸ“ Kiá»ƒm SoÃ¡t Äá»™ DÃ i

| Tham sá»‘ | MÃ´ táº£ | GiÃ¡ trá»‹ Ä‘á» xuáº¥t | LÆ°u Ã½ |
|---------|-------|-----------------|-------|
| `max_length` | Äá»™ dÃ i tá»‘i Ä‘a (tokens) | 100-150 | Pháº£i < Ä‘á»™ dÃ i input |
| `min_length` | Äá»™ dÃ i tá»‘i thiá»ƒu (tokens) | 30-50 | Äáº£m báº£o summary Ä‘á»§ thÃ´ng tin |
| `length_penalty` | Äiá»u chá»‰nh xu hÆ°á»›ng Ä‘á»™ dÃ i | 1.0-2.0 | >1: Æ°u tiÃªn summary dÃ i hÆ¡n |
| `no_repeat_ngram_size` | TrÃ¡nh láº·p láº¡i n-gram | 2-3 | Giáº£m sá»± láº·p láº¡i trong output |

### ğŸ’¡ Best Practices cho Parameters

```python
# VÃ­ dá»¥ cáº¥u hÃ¬nh tá»‘i Æ°u
summary = summarizer(
    text,
    max_length=120,          # KhÃ´ng quÃ¡ dÃ i
    min_length=40,           # Äá»§ thÃ´ng tin
    length_penalty=1.2,      # Khuyáº¿n khÃ­ch summary vá»«a pháº£i
    no_repeat_ngram_size=3,  # TrÃ¡nh láº·p láº¡i
    do_sample=False          # Deterministic output
)
```

## âš ï¸ LÆ°u Ã Quan Trá»ng

### ğŸ”§ Xá»­ LÃ½ Lá»—i ThÆ°á»ng Gáº·p

| Lá»—i | NguyÃªn nhÃ¢n | Giáº£i phÃ¡p |
|-----|-------------|-----------|
| `max_length < input_length` | Input quÃ¡ ngáº¯n so vá»›i max_length | Giáº£m max_length xuá»‘ng |
| Memory Error | Text quÃ¡ dÃ i | Chia nhá» text hoáº·c dÃ¹ng model nhá» hÆ¡n |
| Poor Quality Summary | Model khÃ´ng phÃ¹ há»£p | Thá»­ model khÃ¡c |
| Repetitive Output | KhÃ´ng Ä‘áº·t no_repeat_ngram_size | ThÃªm tham sá»‘ nÃ y |

### ğŸ“ Tokens vs Words

```python
# Hiá»ƒu vá» tokens
text = "Hello world!"
# Tokens cÃ³ thá»ƒ lÃ : ["Hello", " world", "!"]
# 1 token â‰ˆ 0.75 words (tiáº¿ng Anh)
# 1 token â‰ˆ 0.5-1 words (tiáº¿ng Viá»‡t)
```

## ğŸ”„ Quy TrÃ¬nh PhÃ¡t Triá»ƒn

### 1. **PhÃ¢n TÃ­ch YÃªu Cáº§u**
```
Input: Loáº¡i vÄƒn báº£n cáº§n tÃ³m táº¯t?
Output: Äá»™ dÃ i mong muá»‘n?
Quality: Cáº§n Ä‘á»™ chÃ­nh xÃ¡c hay tÃ­nh tá»± nhiÃªn?
```

### 2. **Chá»n PhÆ°Æ¡ng PhÃ¡p**
```
Extractive â† Äá»™ chÃ­nh xÃ¡c cao
Abstractive â† TÃ­nh tá»± nhiÃªn cao
```

### 3. **Chá»n Model**
```python
# Models phá»• biáº¿n
EXTRACTIVE_MODELS = [
    "sshleifer/distilbart-cnn-6-6",
    "google/pegasus-xsum"
]

ABSTRACTIVE_MODELS = [
    "facebook/bart-large-cnn",
    "t5-small",
    "google/pegasus-cnn_dailymail"
]
```

### 4. **Testing & Optimization**
```python
# Test vá»›i sample data
sample_texts = [
    "Short text...",
    "Medium length text...", 
    "Very long text..."
]

for text in sample_texts:
    result = summarizer(text, max_length=100, min_length=30)
    print(f"Length: {len(text)} -> {len(result[0]['summary_text'])}")
```

## ğŸ“ˆ Performance Tips

### ğŸš€ Tá»‘i Æ¯u Hiá»‡u Suáº¥t

1. **Batch Processing**
```python
# Xá»­ lÃ½ nhiá»u text cÃ¹ng lÃºc
texts = ["text1...", "text2...", "text3..."]
summaries = summarizer(texts, max_length=100, min_length=30)
```

2. **Model Caching**
```python
# Cache model Ä‘á»ƒ trÃ¡nh reload
import torch
device = 0 if torch.cuda.is_available() else -1
summarizer = pipeline("summarization", device=device)
```

3. **Memory Management**
```python
# Chia nhá» text dÃ i
def chunk_text(text, max_chunk_size=1000):
    sentences = text.split('. ')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk + sentence) < max_chunk_size:
            current_chunk += sentence + ". "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks
```

## ğŸ¯ Káº¿t Luáº­n

Text Summarization lÃ  cÃ´ng cá»¥ máº¡nh máº½ vá»›i hai phÆ°Æ¡ng phÃ¡p chÃ­nh:
- **Extractive**: Cho Ä‘á»™ chÃ­nh xÃ¡c cao, phÃ¹ há»£p tÃ i liá»‡u quan trá»ng
- **Abstractive**: Cho tÃ­nh tá»± nhiÃªn cao, phÃ¹ há»£p ná»™i dung marketing

Viá»‡c chá»n Ä‘Ãºng phÆ°Æ¡ng phÃ¡p vÃ  cáº¥u hÃ¬nh tham sá»‘ phÃ¹ há»£p sáº½ quyáº¿t Ä‘á»‹nh cháº¥t lÆ°á»£ng summary cuá»‘i cÃ¹ng.
