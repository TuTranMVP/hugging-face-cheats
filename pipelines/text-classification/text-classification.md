# Text Classification v·ªõi Hugging Face

## üìã T·ªïng quan

Text Classification l√† k·ªπ thu·∫≠t machine learning ph√¢n lo·∫°i vƒÉn b·∫£n v√†o c√°c danh m·ª•c ƒë·ªãnh s·∫µn. ƒê√¢y l√† m·ªôt trong nh·ªØng t√°c v·ª• ph·ªï bi·∫øn nh·∫•t trong NLP, ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong th·ª±c t·∫ø.

## üéØ C√°c lo·∫°i Text Classification

### 1. Sentiment Analysis (Ph√¢n t√≠ch C·∫£m x√∫c)

**M·ª•c ƒë√≠ch:** Ph√¢n lo·∫°i vƒÉn b·∫£n d·ª±a tr√™n c·∫£m x√∫c/th√°i ƒë·ªô

| V√≠ d·ª• Input | Label | Confidence |
|-------------|-------|------------|
| "I love pineapple on pizza" | Positive | 0.95 |
| "I dislike pineapple on pizza" | Negative | 0.89 |

**·ª®ng d·ª•ng th·ª±c t·∫ø:**
- Ph√¢n t√≠ch review s·∫£n ph·∫©m
- Monitoring social media
- ƒê√°nh gi√° feedback kh√°ch h√†ng

### 2. Grammatical Correctness (Ki·ªÉm tra Ng·ªØ ph√°p)

**M·ª•c ƒë√≠ch:** Ki·ªÉm tra t√≠nh ƒë√∫ng ƒë·∫Øn v·ªÅ ng·ªØ ph√°p

| V√≠ d·ª• Input | Label | M√¥ t·∫£ |
|-------------|-------|-------|
| "This course is great!" | Acceptable | Ng·ªØ ph√°p ƒë√∫ng |
| "Course is gravy" | Unacceptable | Ng·ªØ ph√°p sai |
| "He eat pizza every day" | LABEL_0 (Incorrect) | Thi·∫øu 's' trong 'eats' |

**·ª®ng d·ª•ng th·ª±c t·∫ø:**
- Grammar checkers
- Language learning tools
- Content editing systems

### 3. QNLI (Question Natural Language Inference)

**M·ª•c ƒë√≠ch:** Ki·ªÉm tra xem premise c√≥ tr·∫£ l·ªùi ƒë∆∞·ª£c question kh√¥ng

| Question | Premise | Label | M√¥ t·∫£ |
|----------|---------|-------|-------|
| "What state is Hollywood in?" | "Hollywood is in California" | Entailment (True) | Premise tr·∫£ l·ªùi ƒë∆∞·ª£c question |
| "What state is Hollywood in?" | "Hollywood is known for movies" | Not Entailment (False) | Premise kh√¥ng tr·∫£ l·ªùi question |

**·ª®ng d·ª•ng th·ª±c t·∫ø:**
- Question-answering systems
- Fact-checking applications
- Search engines

### 4. Dynamic Category Assignment (Ph√¢n lo·∫°i ƒê·ªông)

**M·ª•c ƒë√≠ch:** Ph√¢n lo·∫°i vƒÉn b·∫£n v√†o c√°c danh m·ª•c ƒë·ªãnh s·∫µn

| V√≠ d·ª• Input | Categories | Top Prediction |
|-------------|------------|----------------|
| "I want to know about pricing" | Sales, Marketing, Support | Sales (0.85) |
| "Feature our courses in newsletter" | Marketing, Sales, Support | Support (0.72) |

**·ª®ng d·ª•ng th·ª±c t·∫ø:**
- Content moderation
- Email routing
- Recommendation systems

## üîß Implementation Guide

### Sentiment Analysis
```python
from transformers import pipeline

# T·∫°o pipeline
classifier = pipeline("text-classification", 
                     model="cardiffnlp/twitter-roberta-base-sentiment-latest")

# S·ª≠ d·ª•ng
result = classifier("I love this product!")
print(f"Label: {result[0]['label']}, Score: {result[0]['score']:.3f}")
```

### Grammatical Correctness
```python
from transformers import pipeline

# Model cho grammar checking
grammar_checker = pipeline("text-classification", 
                          model="textattack/roberta-base-CoLA")

result = grammar_checker("He eat pizza every day")
print(f"Grammar: {result[0]['label']}, Confidence: {result[0]['score']:.3f}")
```

### QNLI
```python
from transformers import pipeline

# QNLI pipeline
qnli_classifier = pipeline("text-classification", 
                          model="roberta-large-mnli")

# Format: "question, premise"
text = "What state is Hollywood in?, Hollywood is in California"
result = qnli_classifier(text)
print(f"Entailment: {result[0]['label']}, Score: {result[0]['score']:.3f}")
```

### Zero-shot Classification
```python
from transformers import pipeline

# Zero-shot classifier
classifier = pipeline("zero-shot-classification")

text = "Hey, DataCamp; we would like to feature your courses!"
candidate_labels = ["Marketing", "Sales", "Support"]

result = classifier(text, candidate_labels)
print(f"Top category: {result['labels'][0]}")
print(f"Confidence: {result['scores'][0]:.3f}")
```

## üìä So s√°nh c√°c ph∆∞∆°ng ph√°p

| Lo·∫°i | Use Case | Model Example | Pros | Cons |
|------|----------|---------------|------|------|
| **Sentiment Analysis** | Review analysis | `cardiffnlp/twitter-roberta-base-sentiment` | ‚úÖ Accuracy cao<br>‚úÖ Domain specific | ‚ùå Limited to sentiment only |
| **Grammar Check** | Content editing | `textattack/roberta-base-CoLA` | ‚úÖ Fast inference<br>‚úÖ Clear binary output | ‚ùå No correction suggestions |
| **QNLI** | Q&A systems | `roberta-large-mnli` | ‚úÖ Logical reasoning<br>‚úÖ Fact verification | ‚ùå Requires structured input |
| **Zero-shot** | Dynamic categories | `facebook/bart-large-mnli` | ‚úÖ Flexible categories<br>‚úÖ No retraining needed | ‚ùå Lower accuracy<br>‚ùå Slower inference |

## üöß Th√°ch th·ª©c v√† Gi·∫£i ph√°p

### 1. **Ambiguity (T√≠nh M∆° h·ªì)**
```
‚ùå V·∫•n ƒë·ªÅ: "This is sick!" (c√≥ th·ªÉ l√† t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c)
‚úÖ Gi·∫£i ph√°p: Context-aware models, domain-specific training
```

### 2. **Sarcasm/Irony (Ch√¢m bi·∫øm/M·ªâa mai)**
```
‚ùå V·∫•n ƒë·ªÅ: "Great! Another bug in production üôÑ"
‚úÖ Gi·∫£i ph√°p: Multi-modal analysis (text + emoji), advanced models
```

### 3. **Multilingual Complexity (ƒêa ng√¥n ng·ªØ)**
```
‚ùå V·∫•n ƒë·ªÅ: "S·∫£n ph·∫©m n√†y r·∫•t tuy·ªát v·ªùi!" (Ti·∫øng Vi·ªát)
‚úÖ Gi·∫£i ph√°p: Multilingual models (mBERT, XLM-R), language detection
```

## üîÑ Text Classification Workflow

```mermaid
graph TD
    A[Raw Text Input] --> B{Preprocessing}
    B --> C[Tokenization]
    C --> D[Model Inference]
    D --> E{Task Type}
    
    E -->|Sentiment| F[Positive/Negative/Neutral]
    E -->|Grammar| G[Acceptable/Unacceptable]
    E -->|QNLI| H[Entailment/Not Entailment]
    E -->|Zero-shot| I[Dynamic Categories]
    
    F --> J[Post-processing]
    G --> J
    H --> J
    I --> J
    
    J --> K[Final Results]
    K --> L[Application Logic]
```

## üí° Best Practices

### 1. **Model Selection**
- **Sentiment Analysis**: Ch·ªçn model train tr√™n domain t∆∞∆°ng t·ª± (social media, reviews, etc.)
- **Grammar Check**: S·ª≠ d·ª•ng models train tr√™n CoLA dataset
- **QNLI**: Models train tr√™n MNLI/SNLI datasets
- **Zero-shot**: BART ho·∫∑c T5-based models

### 2. **Performance Optimization**
```python
# Batch processing cho hi·ªáu su·∫•t t·ªët h∆°n
texts = ["Text 1", "Text 2", "Text 3"]
results = classifier(texts, batch_size=8)

# S·ª≠ d·ª•ng GPU n·∫øu c√≥
classifier = pipeline("text-classification", 
                     model="model-name", 
                     device=0)  # GPU device
```

### 3. **Error Handling**
```python
try:
    result = classifier(text)
    if result[0]['score'] < 0.7:
        print("‚ö†Ô∏è Low confidence prediction")
except Exception as e:
    print(f"‚ùå Classification failed: {e}")
```

## üìà Evaluation Metrics

| Metric | Formula | Use Case |
|--------|---------|----------|
| **Accuracy** | `(TP + TN) / Total` | Overall performance |
| **Precision** | `TP / (TP + FP)` | False positive sensitivity |
| **Recall** | `TP / (TP + FN)` | False negative sensitivity |
| **F1-Score** | `2 * (Precision * Recall) / (Precision + Recall)` | Balanced metric |
| **Confidence** | Model output probability | Prediction reliability |

## üéØ Production Considerations

### 1. **Latency Requirements**
- **Real-time**: DistilBERT, MobileBERT (< 100ms)
- **Batch processing**: RoBERTa, BERT (< 1s)
- **High accuracy**: Large models (1-5s acceptable)

### 2. **Scalability**
```python
# Async processing cho high throughput
import asyncio
from transformers import pipeline

async def classify_batch(texts):
    classifier = pipeline("text-classification")
    return classifier(texts)

# Load balancing multiple models
```

### 3. **Monitoring**
- Track prediction confidence scores
- Monitor classification distribution
- Set up alerts for accuracy drops

## üîó Useful Resources

- **Hugging Face Models**: [huggingface.co/models](https://huggingface.co/models?pipeline_tag=text-classification)
- **Datasets**: [huggingface.co/datasets](https://huggingface.co/datasets?task_categories=task_categories:text-classification)
- **Documentation**: [transformers documentation](https://huggingface.co/docs/transformers/tasks/sequence_classification)

---

## üèÉ‚Äç‚ôÇÔ∏è Quick Start

1. **Install dependencies**:
   ```bash
   pip install transformers torch
   ```

2. **Run basic classification**:
   ```python
   from transformers import pipeline
   
   classifier = pipeline("text-classification")
   result = classifier("I love machine learning!")
   print(result)
   ```

3. **Experiment with different tasks** trong c√°c examples ·ªü tr√™n

4. **Integrate v√†o application** c·ªßa b·∫°n v·ªõi proper error handling v√† monitoring
