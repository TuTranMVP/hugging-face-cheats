# ü§ñ Auto Models v√† Tokenizers - H∆∞·ªõng d·∫´n to√†n di·ªán

## üìã T·ªïng quan

Auto Classes trong Hugging Face Transformers cung c·∫•p c√°ch ti·∫øp c·∫≠n linh ho·∫°t v√† m·∫°nh m·∫Ω ƒë·ªÉ l√†m vi·ªác v·ªõi models v√† tokenizers, cho ph√©p ki·ªÉm so√°t chi ti·∫øt h∆°n so v·ªõi Pipelines.

## üîÑ So s√°nh Pipelines vs Auto Classes

| Ti√™u ch√≠ | Pipelines | Auto Classes |
|----------|-----------|--------------|
| **ƒê·ªô d·ªÖ s·ª≠ d·ª•ng** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê R·∫•t ƒë∆°n gi·∫£n | ‚≠ê‚≠ê‚≠ê C·∫ßn hi·ªÉu bi·∫øt |
| **T√≠nh linh ho·∫°t** | ‚≠ê‚≠ê H·∫°n ch·∫ø | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê R·∫•t linh ho·∫°t |
| **Ki·ªÉm so√°t** | ‚≠ê‚≠ê T·ª± ƒë·ªông | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Ki·ªÉm so√°t ho√†n to√†n |
| **T√πy ch·ªânh** | ‚≠ê Kh√≥ t√πy ch·ªânh | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê T√πy ch·ªânh s√¢u |
| **Use case** | Prototype nhanh | Production, Custom workflows |

## üéØ Khi n√†o s·ª≠ d·ª•ng Auto Classes?

### ‚úÖ N√™n s·ª≠ d·ª•ng khi:
- C·∫ßn t√πy ch·ªânh preprocessing chi ti·∫øt
- Mu·ªën ki·ªÉm so√°t tokenization process
- X√¢y d·ª±ng custom pipeline ph·ª©c t·∫°p
- C·∫ßn fine-tune thresholds v√† weights
- T√≠ch h·ª£p v√†o production workflows

### ‚ùå Kh√¥ng c·∫ßn thi·∫øt khi:
- Ch·ªâ c·∫ßn k·∫øt qu·∫£ nhanh cho demo
- Kh√¥ng c·∫ßn t√πy ch·ªânh g√¨ ƒë·∫∑c bi·ªát
- M·ªõi b·∫Øt ƒë·∫ßu h·ªçc Hugging Face

## üèóÔ∏è Ki·∫øn tr√∫c Auto Classes

```mermaid
graph TD
    A[Raw Text] --> B[AutoTokenizer]
    B --> C[Tokenized Input]
    C --> D[AutoModel]
    D --> E[Model Output]
    E --> F[Post-processing]
    F --> G[Final Result]
    
    H[Model Name] --> B
    H --> D
    
    style B fill:#e1f5fe
    style D fill:#f3e5f5
    style A fill:#f1f8e9
    style G fill:#fff3e0
```

## üîß Auto Classes ch√≠nh

### 1. ü§ñ AutoModel Classes

| Class | M·ª•c ƒë√≠ch | V√≠ d·ª• s·ª≠ d·ª•ng |
|-------|----------|---------------|
| `AutoModel` | Base model cho feature extraction | Embedding, representations |
| `AutoModelForSequenceClassification` | Text classification | Sentiment analysis, spam detection |
| `AutoModelForTokenClassification` | Token-level classification | NER, POS tagging |
| `AutoModelForQuestionAnswering` | Question answering | Reading comprehension |
| `AutoModelForMaskedLM` | Masked language modeling | BERT-style pretraining |
| `AutoModelForCausalLM` | Causal language modeling | GPT-style text generation |

### 2. üî§ AutoTokenizer

| T√≠nh nƒÉng | M√¥ t·∫£ | Code example |
|-----------|-------|--------------|
| **Tokenization** | Chia text th√†nh tokens | `tokenizer.tokenize(text)` |
| **Encoding** | Convert tokens to IDs | `tokenizer.encode(text)` |
| **Batch processing** | X·ª≠ l√Ω nhi·ªÅu texts c√πng l√∫c | `tokenizer(texts, padding=True)` |
| **Special tokens** | Th√™m [CLS], [SEP], etc. | `tokenizer(text, add_special_tokens=True)` |

## üíª Code Examples

### V√≠ d·ª• 1: Text Classification c∆° b·∫£n

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# 1. Load model v√† tokenizer
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 2. Prepare input
text = "I love this product!"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

# 3. Model inference
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

# 4. Get results
predicted_class = torch.argmax(predictions, dim=-1)
confidence = torch.max(predictions).item()

print(f"Prediction: {predicted_class.item()}")
print(f"Confidence: {confidence:.4f}")
```

### V√≠ d·ª• 2: Custom Pipeline

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# Load components
model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Create custom pipeline
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

# Use pipeline
texts = [
    "T√¥i r·∫•t th√≠ch s·∫£n ph·∫©m n√†y!",
    "D·ªãch v·ª• kh√°ch h√†ng t·ªá qu√°",
    "S·∫£n ph·∫©m b√¨nh th∆∞·ªùng, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát"
]

results = sentiment_pipeline(texts)
for text, result in zip(texts, results):
    print(f"Text: {text}")
    print(f"Sentiment: {result['label']} ({result['score']:.4f})")
    print("-" * 50)
```

### V√≠ d·ª• 3: Batch Processing v·ªõi Tokenizer

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Batch tokenization
texts = [
    "Hello world!",
    "This is a longer sentence that needs tokenization.",
    "Short text."
]

# Tokenize v·ªõi padding v√† truncation
encoded = tokenizer(
    texts,
    padding=True,           # Pad to same length
    truncation=True,        # Truncate long sequences
    max_length=128,         # Maximum sequence length
    return_tensors="pt"     # Return PyTorch tensors
)

print("Input IDs shape:", encoded['input_ids'].shape)
print("Attention mask shape:", encoded['attention_mask'].shape)

# Decode ƒë·ªÉ xem tokens
for i, text in enumerate(texts):
    tokens = tokenizer.tokenize(text)
    print(f"Original: {text}")
    print(f"Tokens: {tokens}")
    print(f"Token IDs: {encoded['input_ids'][i][:len(tokens)+2]}")  # +2 for [CLS] and [SEP]
    print("-" * 50)
```

## üîÑ Tokenization Process Flow

```mermaid
flowchart LR
    A[Raw Text] --> B[Text Cleaning]
    B --> C[Tokenization]
    C --> D[Token to ID Mapping]
    D --> E[Add Special Tokens]
    E --> F[Padding/Truncation]
    F --> G[Return Tensors]
    
    subgraph "Tokenizer Steps"
        B
        C
        D
        E
        F
    end
    
    style A fill:#e8f5e8
    style G fill:#e8f5e8
    style B fill:#fff3cd
    style C fill:#fff3cd
    style D fill:#fff3cd
    style E fill:#fff3cd
    style F fill:#fff3cd
```

## ‚öôÔ∏è Advanced Techniques

### 1. Custom Preprocessing

```python
from transformers import AutoTokenizer

class CustomTokenizer:
    def __init__(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    def preprocess_vietnamese(self, text):
        """Custom preprocessing cho ti·∫øng Vi·ªát"""
        # Remove extra whitespace
        text = " ".join(text.split())
        
        # Normalize Vietnamese characters
        text = text.replace("ƒë", "d").replace("ƒê", "D")
        
        # Add custom processing here
        return text
    
    def encode_with_preprocessing(self, text, **kwargs):
        """Encode v·ªõi custom preprocessing"""
        processed_text = self.preprocess_vietnamese(text)
        return self.tokenizer(processed_text, **kwargs)

# Usage
custom_tokenizer = CustomTokenizer("vinai/phobert-base")
result = custom_tokenizer.encode_with_preprocessing("Xin ch√†o Vi·ªát Nam!")
```

### 2. Dynamic Thresholding

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

class SentimentClassifier:
    def __init__(self, model_name, custom_thresholds=None):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        
        # Custom thresholds cho t·ª´ng class
        self.thresholds = custom_thresholds or {
            'NEGATIVE': 0.6,
            'NEUTRAL': 0.3,
            'POSITIVE': 0.7
        }
    
    def predict_with_confidence(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # Apply custom thresholds
        predicted_class = torch.argmax(probabilities, dim=-1).item()
        confidence = torch.max(probabilities).item()
        
        labels = ['NEGATIVE', 'NEUTRAL', 'POSITIVE']  # Adjust based on your model
        predicted_label = labels[predicted_class]
        
        # Check if confidence meets threshold
        meets_threshold = confidence >= self.thresholds[predicted_label]
        
        return {
            'label': predicted_label,
            'confidence': confidence,
            'meets_threshold': meets_threshold,
            'all_scores': {label: prob.item() for label, prob in zip(labels, probabilities[0])}
        }

# Usage
classifier = SentimentClassifier(
    "cardiffnlp/twitter-roberta-base-sentiment-latest",
    custom_thresholds={'NEGATIVE': 0.8, 'NEUTRAL': 0.5, 'POSITIVE': 0.7}
)

result = classifier.predict_with_confidence("This product is amazing!")
print(result)
```

## üìä Performance Optimization

### Best Practices Summary

| Technique | M√¥ t·∫£ | Impact |
|-----------|-------|---------|
| **Batch Processing** | Process nhi·ªÅu texts c√πng l√∫c | üöÄ 3-5x faster |
| **GPU Acceleration** | S·ª≠ d·ª•ng CUDA n·∫øu c√≥ | üöÄ 10-50x faster |
| **Model Quantization** | Gi·∫£m precision ƒë·ªÉ tƒÉng t·ªëc | ‚ö° 2-4x faster, √≠t RAM |
| **Caching** | Cache tokenized inputs | üíæ Gi·∫£m repeated computation |
| **Truncation** | Gi·ªõi h·∫°n sequence length | ‚ö° Gi·∫£m memory usage |

### Performance Code Example

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class OptimizedClassifier:
    def __init__(self, model_name, device=None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()  # Set to evaluation mode
        
    def predict_batch(self, texts, batch_size=32):
        """Batch prediction cho performance t·ªët h∆°n"""
        results = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            # Tokenize batch
            inputs = self.tokenizer(
                batch_texts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            ).to(self.device)
            
            # Predict
            with torch.no_grad():
                outputs = self.model(**inputs)
                probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
                
            # Process results
            for j, probs in enumerate(probabilities):
                predicted_class = torch.argmax(probs).item()
                confidence = torch.max(probs).item()
                
                results.append({
                    'text': batch_texts[j],
                    'prediction': predicted_class,
                    'confidence': confidence
                })
        
        return results

# Usage
classifier = OptimizedClassifier("distilbert-base-uncased-finetuned-sst-2-english")

texts = [
    "I love this!",
    "This is terrible",
    "Average product",
    # ... more texts
]

results = classifier.predict_batch(texts, batch_size=16)
```

## üö® Common Issues v√† Solutions

### Issue 1: Tokenizer Mismatch
```python
# ‚ùå Wrong - Using different tokenizers
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")  # Wrong!

# ‚úÖ Correct - Matching tokenizer v√† model
model_name = "bert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)  # Same model name
```

### Issue 2: Memory Issues v·ªõi Long Sequences
```python
# ‚ùå Memory intensive
inputs = tokenizer(very_long_text, return_tensors="pt")  # No truncation

# ‚úÖ Memory efficient
inputs = tokenizer(
    very_long_text, 
    return_tensors="pt",
    truncation=True,
    max_length=512,  # Reasonable limit
    padding=True
)
```

### Issue 3: Slow Inference
```python
# ‚ùå Slow - Processing one by one
for text in texts:
    result = model(tokenizer(text, return_tensors="pt"))

# ‚úÖ Fast - Batch processing
inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
results = model(**inputs)
```

## üìù Cheat Sheet

### Quick Commands

```python
# Load model v√† tokenizer
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "your-model-name"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Single prediction
text = "Your text here"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
prediction = torch.argmax(outputs.logits, dim=-1)

# Batch prediction
texts = ["Text 1", "Text 2", "Text 3"]
inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
outputs = model(**inputs)
predictions = torch.argmax(outputs.logits, dim=-1)
```

## üéì K·∫øt lu·∫≠n

Auto Classes cung c·∫•p s·ª± linh ho·∫°t v√† ki·ªÉm so√°t cao cho vi·ªác x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng NLP production-ready. Vi·ªác hi·ªÉu r√µ c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa AutoModel v√† AutoTokenizer s·∫Ω gi√∫p b·∫°n:

- ‚úÖ T√πy ch·ªânh preprocessing theo nhu c·∫ßu c·ª• th·ªÉ
- ‚úÖ T·ªëi ∆∞u performance cho production
- ‚úÖ X√¢y d·ª±ng custom workflows ph·ª©c t·∫°p
- ‚úÖ Debug v√† troubleshoot hi·ªáu qu·∫£

### Next Steps:
1. Th·ª±c h√†nh v·ªõi c√°c model kh√°c nhau
2. T√¨m hi·ªÉu v·ªÅ fine-tuning
3. Kh√°m ph√° advanced tokenization techniques
4. H·ªçc c√°ch deploy models l√™n production

---

**üí° Pro Tip**: Lu√¥n s·ª≠ d·ª•ng c√πng tokenizer v·ªõi model ƒë·ªÉ ƒë·∫£m b·∫£o consistency v√† accuracy t·ªët nh·∫•t!
