#!/usr/bin/env python3
"""
AI Interview Agent - Hugging Face Knowledge Assessment
CLI Chatbox ƒë·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ file .md v√† m√¥ ph·ªèng ph·ªèng v·∫•n
"""

from dataclasses import dataclass
from datetime import datetime
import os
from pathlib import Path
import random
import re
import sys
from typing import Any, Dict, List, Optional

from bs4 import BeautifulSoup
import click
import colorama
import markdown
import requests
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from rich.prompt import Confirm, Prompt
from rich.table import Table

# Kh·ªüi t·∫°o colorama cho Windows
colorama.init(autoreset=True)

# Kh·ªüi t·∫°o Rich console
console = Console()


@dataclass
class Question:
    """C·∫•u tr√∫c d·ªØ li·ªáu cho c√¢u h·ªèi"""

    id: str
    title: str
    description: str
    question: str
    options: List[str]
    correct_answer: str
    explanation: str
    difficulty: str = 'medium'
    topic: str = 'general'


@dataclass
class Knowledge:
    """C·∫•u tr√∫c d·ªØ li·ªáu cho ki·∫øn th·ª©c"""

    title: str
    content: str
    source_file: str
    sections: List[str]
    keywords: List[str]
    folder: str = ''
    file_type: str = 'markdown'
    importance_score: float = 1.0


@dataclass
class WorkspaceConfig:
    """C·∫•u h√¨nh workspace loading"""

    include_folders: Optional[List[str]] = None  # Folders to include
    exclude_folders: Optional[List[str]] = None  # Folders to exclude
    file_patterns: Optional[List[str]] = None  # File patterns to match
    max_file_size: int = 1024 * 1024  # Max file size in bytes
    enable_code_analysis: bool = True  # Analyze .py files
    enable_auto_summary: bool = True  # Auto generate summaries


class WorkspaceLoader:
    """N√¢ng c·∫•p ƒë·ªÉ n·∫°p to√†n b·ªô workspace th√¥ng minh"""

    def __init__(self, config: Optional[WorkspaceConfig] = None):
        self.config = config or WorkspaceConfig()
        self.md_parser = markdown.Markdown(extensions=['extra', 'toc'])
        self.supported_extensions = {'.md', '.txt', '.py', '.json', '.yml', '.yaml'}

    def discover_workspace(self, root_path: str) -> Dict[str, List[str]]:
        """Kh√°m ph√° c·∫•u tr√∫c workspace"""
        workspace_structure = {
            'folders': [],
            'markdown_files': [],
            'python_files': [],
            'config_files': [],
            'question_files': [],
            'knowledge_files': [],
        }

        root = Path(root_path)

        for path in root.rglob('*'):
            if path.is_dir():
                if self._should_include_folder(str(path.relative_to(root))):
                    workspace_structure['folders'].append(str(path))
            elif path.is_file():
                rel_path = str(path.relative_to(root))
                if self._should_include_file(rel_path):
                    if path.suffix == '.md':
                        if 'question' in path.name.lower():
                            workspace_structure['question_files'].append(str(path))
                        else:
                            workspace_structure['knowledge_files'].append(str(path))
                        workspace_structure['markdown_files'].append(str(path))
                    elif path.suffix == '.py':
                        workspace_structure['python_files'].append(str(path))
                    elif path.suffix in {'.json', '.yml', '.yaml'}:
                        workspace_structure['config_files'].append(str(path))

        return workspace_structure

    def load_workspace_content(
        self, root_path: str, selected_folders: Optional[List[str]] = None
    ) -> List[Knowledge]:
        """N·∫°p n·ªôi dung workspace v·ªõi t√πy ch·ªçn folder"""
        knowledge_base = []

        if selected_folders:
            # Load specific folders
            for folder in selected_folders:
                folder_path = Path(root_path) / folder
                if folder_path.exists():
                    knowledge_base.extend(
                        self._load_folder_content(folder_path, folder)
                    )
        else:
            # Load all workspace
            workspace_structure = self.discover_workspace(root_path)

            # Load markdown files
            for md_file in workspace_structure['knowledge_files']:
                knowledge = self._load_markdown_file(md_file)
                if knowledge:
                    knowledge_base.append(knowledge)

            # Load Python files if enabled
            if self.config.enable_code_analysis:
                for py_file in workspace_structure['python_files']:
                    knowledge = self._load_python_file(py_file)
                    if knowledge:
                        knowledge_base.append(knowledge)

        return knowledge_base

    def _load_folder_content(
        self, folder_path: Path, folder_name: str
    ) -> List[Knowledge]:
        """N·∫°p n·ªôi dung t·ª´ m·ªôt folder c·ª• th·ªÉ"""
        knowledge_list = []

        for file_path in folder_path.rglob('*'):
            if file_path.is_file() and file_path.suffix in self.supported_extensions:
                if file_path.suffix == '.md':
                    knowledge = self._load_markdown_file(str(file_path))
                elif file_path.suffix == '.py':
                    knowledge = self._load_python_file(str(file_path))
                else:
                    knowledge = self._load_text_file(str(file_path))

                if knowledge:
                    knowledge.folder = folder_name
                    knowledge_list.append(knowledge)

        return knowledge_list

    def _load_markdown_file(self, file_path: str) -> Optional[Knowledge]:
        """N√¢ng c·∫•p load markdown v·ªõi ph√¢n t√≠ch s√¢u h∆°n"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()

            # Convert markdown to HTML for better parsing
            html_content = self.md_parser.convert(content)
            _soup = BeautifulSoup(html_content, 'html.parser')

            # Extract title
            title = self._extract_title(content, file_path)

            # Extract sections
            sections = self._extract_sections(content)

            # Extract keywords
            keywords = self._extract_keywords(content)

            # Calculate importance score
            importance_score = self._calculate_importance_score(content, file_path)

            return Knowledge(
                title=title,
                content=content,
                source_file=file_path,
                sections=sections,
                keywords=keywords,
                folder=str(Path(file_path).parent.name),
                file_type='markdown',
                importance_score=importance_score,
            )

        except Exception as e:
            console.print(f'[red]L·ªói khi ƒë·ªçc file {file_path}: {e}[/red]')
            return None

    def _load_python_file(self, file_path: str) -> Optional[Knowledge]:
        """N·∫°p v√† ph√¢n t√≠ch file Python"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()

            # Extract docstrings and comments
            title = f'Python Code: {Path(file_path).name}'

            # Extract function and class definitions
            sections = self._extract_python_elements(content)

            # Extract technical keywords
            keywords = self._extract_python_keywords(content)

            # Create summarized content
            summary_content = self._create_python_summary(content)

            return Knowledge(
                title=title,
                content=summary_content,
                source_file=file_path,
                sections=sections,
                keywords=keywords,
                folder=str(Path(file_path).parent.name),
                file_type='python',
                importance_score=0.8,  # Slightly lower than markdown
            )

        except Exception as e:
            console.print(f'[red]L·ªói khi ƒë·ªçc file Python {file_path}: {e}[/red]')
            return None

    def _load_text_file(self, file_path: str) -> Optional[Knowledge]:
        """N·∫°p file text th√¥ng th∆∞·ªùng"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()

            if len(content) > self.config.max_file_size:
                content = (
                    content[: self.config.max_file_size] + '...\n[Content truncated]'
                )

            title = f'Text File: {Path(file_path).name}'
            keywords = self._extract_keywords(content)

            return Knowledge(
                title=title,
                content=content,
                source_file=file_path,
                sections=[],
                keywords=keywords,
                folder=str(Path(file_path).parent.name),
                file_type='text',
                importance_score=0.5,
            )

        except Exception as e:
            console.print(f'[red]L·ªói khi ƒë·ªçc file text {file_path}: {e}[/red]')
            return None

    def _should_include_folder(self, folder_path: str) -> bool:
        """Ki·ªÉm tra c√≥ n√™n include folder kh√¥ng"""
        # Exclude common unnecessary folders
        exclude_defaults = {
            '.git',
            '__pycache__',
            '.vscode',
            'node_modules',
            '.pytest_cache',
        }

        folder_name = Path(folder_path).name
        if folder_name in exclude_defaults:
            return False

        if self.config.exclude_folders:
            if any(excluded in folder_path for excluded in self.config.exclude_folders):
                return False

        if self.config.include_folders:
            return any(
                included in folder_path for included in self.config.include_folders
            )

        return True

    def _should_include_file(self, file_path: str) -> bool:
        """Ki·ªÉm tra c√≥ n√™n include file kh√¥ng"""
        path = Path(file_path)

        # Check file size
        try:
            if path.stat().st_size > self.config.max_file_size:
                return False
        except:  # noqa: E722
            return False

        # Check extension
        if path.suffix not in self.supported_extensions:
            return False

        # Check patterns
        if self.config.file_patterns:
            return any(pattern in file_path for pattern in self.config.file_patterns)

        return True

    def _extract_title(self, content: str, file_path: str) -> str:
        """Tr√≠ch xu·∫•t title th√¥ng minh"""
        # Try to find first H1 heading
        h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if h1_match:
            return h1_match.group(1).strip()

        # Try to find title in front matter
        frontmatter_match = re.search(r'^---\s*\ntitle:\s*(.+)$', content, re.MULTILINE)
        if frontmatter_match:
            return frontmatter_match.group(1).strip()

        # Use filename as fallback
        return Path(file_path).stem.replace('_', ' ').replace('-', ' ').title()

    def _extract_sections(self, content: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c sections t·ª´ markdown"""
        sections = []

        # Find all headings
        heading_pattern = r'^(#{1,6})\s+(.+)$'
        for match in re.finditer(heading_pattern, content, re.MULTILINE):
            level = len(match.group(1))
            title = match.group(2).strip()
            sections.append(f'{"  " * (level - 1)}{title}')

        return sections

    def _extract_keywords(self, content: str) -> List[str]:
        """Tr√≠ch xu·∫•t keywords th√¥ng minh"""
        keywords = set()

        # Technical terms (capitalize words)
        tech_terms = re.findall(r'\b[A-Z][a-z]+(?:[A-Z][a-z]*)*\b', content)
        keywords.update(tech_terms[:20])

        # Common ML/AI terms
        ml_terms = [
            'hugging face',
            'transformers',
            'model',
            'pipeline',
            'tokenizer',
            'dataset',
            'training',
            'inference',
            'api',
            'nlp',
            'llm',
            'bert',
            'gpt',
            'embedding',
            'fine-tuning',
            'classification',
            'summarization',
        ]

        content_lower = content.lower()
        for term in ml_terms:
            if term in content_lower:
                keywords.add(term)

        # Extract quoted terms
        quoted_terms = re.findall(r'["\']([^"\']+)["\']', content)
        keywords.update(
            [term for term in quoted_terms if len(term) > 2 and len(term) < 30]
        )

        return list(keywords)[:30]  # Limit to 30 keywords

    def _calculate_importance_score(self, content: str, file_path: str) -> float:
        """T√≠nh ƒëi·ªÉm quan tr·ªçng c·ªßa file"""
        score = 1.0

        # File name indicators
        filename = Path(file_path).name.lower()
        if 'readme' in filename:
            score += 0.5
        if 'introduction' in filename:
            score += 0.3
        if 'question' in filename:
            score += 0.4

        # Content quality indicators
        if len(content) > 1000:
            score += 0.2
        if '```' in content:  # Has code blocks
            score += 0.1
        if content.count('#') > 3:  # Well structured
            score += 0.1

        return min(score, 2.0)  # Cap at 2.0

    def _extract_python_elements(self, content: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c elements t·ª´ Python code"""
        elements = []

        # Find functions
        func_pattern = r'def\s+(\w+)\s*\([^)]*\):'
        for match in re.finditer(func_pattern, content):
            elements.append(f'Function: {match.group(1)}')

        # Find classes
        class_pattern = r'class\s+(\w+)(?:\([^)]*\))?:'
        for match in re.finditer(class_pattern, content):
            elements.append(f'Class: {match.group(1)}')

        return elements

    def _extract_python_keywords(self, content: str) -> List[str]:
        """Tr√≠ch xu·∫•t keywords t·ª´ Python code"""
        keywords = set()

        # Import statements
        import_pattern = r'(?:from\s+(\S+)\s+)?import\s+([^#\n]+)'
        for match in re.finditer(import_pattern, content):
            if match.group(1):
                keywords.add(match.group(1))
            imports = match.group(2).split(',')
            keywords.update([imp.strip() for imp in imports])

        # Function and class names
        func_class_pattern = r'(?:def|class)\s+(\w+)'
        for match in re.finditer(func_class_pattern, content):
            keywords.add(match.group(1))

        return list(keywords)[:20]

    def _create_python_summary(self, content: str) -> str:
        """T·∫°o summary cho Python file"""
        lines = content.split('\n')
        summary_lines = []

        # Add docstring if exists
        if '"""' in content:
            in_docstring = False
            for line in lines:
                if '"""' in line and not in_docstring:
                    in_docstring = True
                    summary_lines.append(line)
                elif '"""' in line and in_docstring:
                    summary_lines.append(line)
                    break
                elif in_docstring:
                    summary_lines.append(line)

        # Add function/class definitions
        for line in lines:
            if line.strip().startswith(('def ', 'class ', 'import ', 'from ')):
                summary_lines.append(line)

        return '\n'.join(summary_lines[:50])  # Limit to 50 lines


class MarkdownParser:
    """Parser ƒë·ªÉ ph√¢n t√≠ch file markdown"""

    def __init__(self):
        self.md = markdown.Markdown(extensions=['extra', 'toc'])

    def parse_questions(self, file_path: str) -> List[Question]:
        """Ph√¢n t√≠ch file markdown ch·ª©a c√¢u h·ªèi"""
        questions = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # T√°ch c√°c c√¢u h·ªèi b·∫±ng d·∫•u ---
            question_blocks = content.split('---')

            for block in question_blocks:
                if 'exercise_' in block and (
                    '**Question:**' in block or '**C√¢u h·ªèi:**' in block
                ):
                    question = self._parse_question_block(block)
                    if question:
                        questions.append(question)

        except Exception as e:
            console.print(f'[red]L·ªói khi ƒë·ªçc file {file_path}: {e}[/red]')

        return questions

    def _parse_question_block(self, block: str) -> Optional[Question]:
        """Ph√¢n t√≠ch m·ªôt block c√¢u h·ªèi"""
        try:
            # T√¨m exercise ID
            exercise_match = re.search(r'exercise_(\w+)', block)
            if not exercise_match:
                return None

            exercise_id = exercise_match.group(1)

            # T√¨m title (h·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát)
            title_match = re.search(r'\*\*(?:Title|Ti√™u ƒë·ªÅ):\*\*\s*(.+)', block)
            title = (
                title_match.group(1).strip()
                if title_match
                else f'Question {exercise_id}'
            )

            # T√¨m description (h·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát)
            desc_match = re.search(r'\*\*(?:Description|M√¥ t·∫£):\*\*\s*(.+)', block)
            description = desc_match.group(1).strip() if desc_match else ''

            # T√¨m question (h·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát)
            question_match = re.search(r'\*\*(?:Question|C√¢u h·ªèi):\*\*\s*(.+)', block)
            question = question_match.group(1).strip() if question_match else ''

            # T√¨m options (h·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát)
            options_section = re.search(
                r'\*\*(?:Options|C√°c l·ª±a ch·ªçn):\*\*\s*\n((?:- .+\n?)+)', block
            )
            options = []
            if options_section:
                option_lines = options_section.group(1).strip().split('\n')
                options = [
                    line.strip('- ').strip()
                    for line in option_lines
                    if line.strip().startswith('- ')
                ]

            # T√¨m correct answer (h·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát)
            answer_match = re.search(
                r'\*\*(?:Correct Answer|ƒê√°p √°n ƒë√∫ng):\*\*\s*(.+)', block
            )
            correct_answer = answer_match.group(1).strip() if answer_match else ''

            # T√¨m explanation (h·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát)
            explanation_match = re.search(
                r'\*\*(?:Explanation|Gi·∫£i th√≠ch):\*\*\s*(.+)', block
            )
            explanation = (
                explanation_match.group(1).strip() if explanation_match else ''
            )

            return Question(
                id=exercise_id,
                title=title,
                description=description,
                question=question,
                options=options,
                correct_answer=correct_answer,
                explanation=explanation,
            )

        except Exception as e:
            console.print(f'[red]L·ªói khi ph√¢n t√≠ch c√¢u h·ªèi: {e}[/red]')
            return None

    def parse_knowledge(self, file_path: str) -> Knowledge:
        """Ph√¢n t√≠ch file markdown ch·ª©a ki·∫øn th·ª©c"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Parse markdown
            html = self.md.convert(content)
            soup = BeautifulSoup(html, 'html.parser')

            # L·∫•y title (h1 ƒë·∫ßu ti√™n)
            title_tag = soup.find('h1')
            title = title_tag.get_text().strip() if title_tag else Path(file_path).stem

            # L·∫•y c√°c section (h2, h3)
            sections = []
            for heading in soup.find_all(['h2', 'h3']):
                sections.append(heading.get_text().strip())

            # T·∫°o keywords t·ª´ headings v√† content
            keywords = []
            text_content = soup.get_text()

            # Th√™m t·ª´ kh√≥a t·ª´ headings
            keywords.extend([s.lower() for s in sections])

            # T√¨m t·ª´ kh√≥a quan tr·ªçng (c√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng NLP)
            important_words = re.findall(
                r'\b(?:Hugging Face|LLM|Model|API|Hub|Dataset|Fine-tuning|Transformer|GPT|BERT|Llama)\b',
                text_content,
                re.IGNORECASE,
            )
            keywords.extend([w.lower() for w in important_words])

            return Knowledge(
                title=title,
                content=content,
                source_file=file_path,
                sections=sections,
                keywords=list(set(keywords)),  # Remove duplicates
            )

        except Exception as e:
            console.print(f'[red]L·ªói khi ƒë·ªçc file {file_path}: {e}[/red]')
            return None  # type: ignore


@dataclass
class AIResponse:
    """C·∫•u tr√∫c ph·∫£n h·ªìi t·ª´ AI"""

    content: str
    source: str = 'ai'
    confidence: float = 0.0
    thinking_process: str = ''
    knowledge_used: Optional[List[str]] = None

    def __post_init__(self):
        if self.knowledge_used is None:
            self.knowledge_used = []


class OllamaAI:
    """T√≠ch h·ª£p AI Ollama llama3:8b"""

    def __init__(
        self, model_name: str = 'llama3:8b', base_url: str = 'http://localhost:11434'
    ):
        self.model_name = model_name
        self.base_url = base_url
        self.is_available = False
        self.session = requests.Session()
        self._check_availability()

    def _check_availability(self):
        """Ki·ªÉm tra Ollama c√≥ kh·∫£ d·ª•ng kh√¥ng"""
        try:
            response = self.session.get(f'{self.base_url}/api/tags', timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                available_models = [m['name'] for m in models]
                if self.model_name in available_models:
                    self.is_available = True
                    console.print(f'[green]‚úì Ollama {self.model_name} kh·∫£ d·ª•ng[/green]')
                else:
                    console.print(
                        f'[yellow]‚ö† Model {self.model_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t[/yellow]'
                    )
                    console.print(
                        f'[dim]C√≥ th·ªÉ c√†i ƒë·∫∑t: ollama pull {self.model_name}[/dim]'
                    )
            else:
                console.print('[yellow]‚ö† Ollama server kh√¥ng ph·∫£n h·ªìi[/yellow]')
        except requests.RequestException:
            console.print(
                '[yellow]‚ö† Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Ollama (http://localhost:11434)[/yellow]'
            )
            console.print('[dim]H√£y ch·∫Øc ch·∫Øn Ollama ƒë√£ ch·∫°y: ollama serve[/dim]')

    def generate_response(
        self, prompt: str, context: str = '', max_tokens: int = 500
    ) -> AIResponse:
        """T·∫°o ph·∫£n h·ªìi t·ª´ AI"""
        if not self.is_available:
            return AIResponse(
                content='‚ùå AI Ollama kh√¥ng kh·∫£ d·ª•ng. S·ª≠ d·ª•ng ch·∫ø ƒë·ªô rule-based.',
                source='fallback',
                confidence=0.0,
            )

        try:
            # T·∫°o prompt template
            system_prompt = self._create_system_prompt(context)
            full_prompt = f'{system_prompt}\n\nQuestion: {prompt}\n\nAnswer:'

            # G·ªçi API Ollama
            payload = {
                'model': self.model_name,
                'prompt': full_prompt,
                'stream': False,
                'options': {
                    'temperature': 0.7,
                    'top_k': 40,
                    'top_p': 0.9,
                    'num_predict': max_tokens,
                },
            }

            response = self.session.post(
                f'{self.base_url}/api/generate', json=payload, timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                ai_response = result.get('response', '').strip()

                # Parse response ƒë·ªÉ extract thinking process
                thinking_process = ''
                knowledge_used = []

                if '<thinking>' in ai_response:
                    thinking_match = re.search(
                        r'<thinking>(.*?)</thinking>', ai_response, re.DOTALL
                    )
                    if thinking_match:
                        thinking_process = thinking_match.group(1).strip()
                        ai_response = re.sub(
                            r'<thinking>.*?</thinking>',
                            '',
                            ai_response,
                            flags=re.DOTALL,
                        ).strip()

                # T√≠nh confidence d·ª±a tr√™n ƒë·ªô d√†i v√† ch·∫•t l∆∞·ª£ng response
                confidence = min(0.9, len(ai_response) / 300)

                return AIResponse(
                    content=ai_response,
                    source='ollama',
                    confidence=confidence,
                    thinking_process=thinking_process,
                    knowledge_used=knowledge_used,
                )
            else:
                console.print(f'[red]L·ªói API Ollama: {response.status_code}[/red]')
                return AIResponse(
                    content='‚ùå L·ªói khi g·ªçi API Ollama', source='error', confidence=0.0
                )

        except requests.RequestException as e:
            console.print(f'[red]L·ªói k·∫øt n·ªëi Ollama: {e}[/red]')
            return AIResponse(
                content='‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Ollama',
                source='error',
                confidence=0.0,
            )

    def _create_system_prompt(self, context: str) -> str:
        """T·∫°o system prompt cho AI"""
        return f"""B·∫°n l√† AI Assistant chuy√™n v·ªÅ Hugging Face v√† Machine Learning.

Context t·ª´ Knowledge Base:
{context}

QUAN TR·ªåNG - Quy t·∫Øc tr·∫£ l·ªùi:
1. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn v√† ch√≠nh x√°c
2. KH√îNG s·ª≠ d·ª•ng markdown formatting (**, *, `, #, etc.)
3. S·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ l√†m cho c√¢u tr·∫£ l·ªùi sinh ƒë·ªông
4. Cung c·∫•p th√¥ng tin th·ª±c t·∫ø d·ª±a tr√™n context
5. N·∫øu c·∫ßn suy lu·∫≠n, bao quanh trong <thinking></thinking>
6. Tr·∫£ l·ªùi tr·ª±c ti·∫øp, tr√°nh t·ª´ ng·ªØ th·ª´a

Format tr·∫£ l·ªùi mong mu·ªën:
- C√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp v·ªõi emoji
- Danh s√°ch d√πng d·∫•u ‚Ä¢ thay v√¨ s·ªë
- V√≠ d·ª• c·ª• th·ªÉ khi c√≥ th·ªÉ
- T·ªëi ƒëa 3-4 √Ω ch√≠nh

V√≠ d·ª• t·ªët:
ü§ñ Hugging Face l√† n·ªÅn t·∫£ng AI m·ªü v·ªõi c√°c t√≠nh nƒÉng:
‚Ä¢ Model Hub: L∆∞u tr·ªØ h√†ng ng√†n models
‚Ä¢ Datasets: B·ªô s∆∞u t·∫≠p d·ªØ li·ªáu training
‚Ä¢ Spaces: Demo ·ª©ng d·ª•ng AI
‚Ä¢ Transformers: Th∆∞ vi·ªán Python d·ªÖ s·ª≠ d·ª•ng

V√≠ d·ª• tr√°nh:
**Hugging Face** l√† m·ªôt *platform* quan tr·ªçng...

H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu v√† th·ª±c t·∫ø."""


class InterviewAgent:
    """AI Agent ƒë·ªÉ m√¥ ph·ªèng ph·ªèng v·∫•n - Enhanced v·ªõi workspace loading"""

    def __init__(self):
        self.questions = []
        self.knowledge_base = []
        self.current_question = None
        self.score = 0
        self.answered_questions = []
        self.session_stats = {
            'total_questions': 0,
            'correct_answers': 0,
            'start_time': None,
            'end_time': None,
        }
        self.workspace_loader = WorkspaceLoader()
        self.workspace_structure = {}

    def load_workspace(
        self, root_path: str, config: Optional[WorkspaceConfig] = None
    ) -> Dict[str, Any]:
        """N·∫°p to√†n b·ªô workspace v·ªõi configuration"""
        if config:
            self.workspace_loader.config = config

        console.print(f'\n[bold blue]üîç Kh√°m ph√° workspace: {root_path}[/bold blue]')

        # Discover workspace structure
        self.workspace_structure = self.workspace_loader.discover_workspace(root_path)

        # Display workspace structure
        self._display_workspace_structure()

        # Ask user which folders to load
        selected_folders = self._select_folders_to_load()

        # Load content
        with Progress(
            SpinnerColumn(),
            TextColumn('[progress.description]{task.description}'),
            BarColumn(),
            console=console,
        ) as progress:
            task = progress.add_task('ƒêang n·∫°p workspace...', total=100)

            # Load knowledge base
            progress.update(task, description='N·∫°p knowledge base...', completed=20)
            self.knowledge_base = self.workspace_loader.load_workspace_content(
                root_path, selected_folders
            )

            # Load questions from discovered question files
            progress.update(task, description='N·∫°p c√¢u h·ªèi...', completed=60)
            parser = MarkdownParser()
            for question_file in self.workspace_structure['question_files']:
                questions = parser.parse_questions(question_file)
                self.questions.extend(questions)

            progress.update(task, description='Ho√†n th√†nh!', completed=100)

        # Display loading results
        self._display_loading_results()

        return {
            'knowledge_count': len(self.knowledge_base),
            'question_count': len(self.questions),
            'folders_loaded': selected_folders or 'all',
            'workspace_structure': self.workspace_structure,
        }

    def _display_workspace_structure(self):
        """Hi·ªÉn th·ªã c·∫•u tr√∫c workspace"""
        table = Table(title='üìÅ C·∫•u tr√∫c Workspace', title_style='bold blue')
        table.add_column('Lo·∫°i', style='cyan', width=20)
        table.add_column('S·ªë l∆∞·ª£ng', justify='right', style='magenta')
        table.add_column('Chi ti·∫øt', style='white')

        table.add_row(
            'üìÇ Folders',
            str(len(self.workspace_structure['folders'])),
            f'C√≥ th·ªÉ ch·ªçn: {len(self.workspace_structure["folders"])}',
        )
        table.add_row(
            'üìù Markdown Files',
            str(len(self.workspace_structure['markdown_files'])),
            'T√†i li·ªáu v√† h∆∞·ªõng d·∫´n',
        )
        table.add_row(
            '‚ùì Question Files',
            str(len(self.workspace_structure['question_files'])),
            'Ng√¢n h√†ng c√¢u h·ªèi',
        )
        table.add_row(
            'üêç Python Files',
            str(len(self.workspace_structure['python_files'])),
            'Source code v√† examples',
        )
        table.add_row(
            '‚öôÔ∏è Config Files',
            str(len(self.workspace_structure['config_files'])),
            'Configuration files',
        )

        console.print(table)

    def _select_folders_to_load(self) -> Optional[List[str]]:
        """Cho ph√©p user ch·ªçn folders ƒë·ªÉ load"""
        if not self.workspace_structure['folders']:
            return None

        # Extract unique folder names from full paths
        folder_names = set()
        for folder_path in self.workspace_structure['folders']:
            # Get relative folder names
            parts = Path(folder_path).parts
            if len(parts) > 1:  # Not root
                folder_names.add(parts[-1])  # Last part (folder name)

        folder_list = sorted(folder_names)

        if not folder_list:
            return None

        console.print(
            f'\n[bold yellow]üìÇ C√≥ {len(folder_list)} folders c√≥ th·ªÉ ch·ªçn:[/bold yellow]'
        )

        # Display options
        table = Table(show_header=True, header_style='bold magenta')
        table.add_column('S·ªë', style='cyan', width=5)
        table.add_column('Folder', style='white')
        table.add_column('M√¥ t·∫£', style='dim')

        folder_descriptions = {
            'getting-started': 'H∆∞·ªõng d·∫´n c∆° b·∫£n',
            'pipelines': 'C√°c pipeline chuy√™n bi·ªát',
            'text-classification': 'Ph√¢n lo·∫°i vƒÉn b·∫£n',
            'text-summarization': 'T√≥m t·∫Øt vƒÉn b·∫£n',
            'datasets': 'X·ª≠ l√Ω d·ªØ li·ªáu',
            '__pycache__': 'Cache files (kh√¥ng khuy·∫øn kh√≠ch)',
        }

        for i, folder in enumerate(folder_list, 1):
            description = folder_descriptions.get(folder, 'N·ªôi dung kh√°c')
            table.add_row(str(i), folder, description)

        table.add_row('0', '[ALL]', 'T·∫•t c·∫£ folders')

        console.print(table)

        # Get user choice
        try:
            choice = Prompt.ask(
                '\n[bold]Ch·ªçn folders (v√≠ d·ª•: 1,2,3 ho·∫∑c 0 cho t·∫•t c·∫£)[/bold]',
                default='0',
            )

            if choice == '0':
                console.print('[green]‚úì S·∫Ω n·∫°p t·∫•t c·∫£ folders[/green]')
                return None  # Load all

            # Parse selected folders
            selected_indices = [
                int(x.strip()) for x in choice.split(',') if x.strip().isdigit()
            ]
            selected_folders = [
                folder_list[i - 1]
                for i in selected_indices
                if 1 <= i <= len(folder_list)
            ]

            if selected_folders:
                console.print(f'[green]‚úì S·∫Ω n·∫°p: {", ".join(selected_folders)}[/green]')
                return selected_folders
            else:
                console.print(
                    '[yellow]‚ö† Kh√¥ng c√≥ l·ª±a ch·ªçn h·ª£p l·ªá, s·∫Ω n·∫°p t·∫•t c·∫£[/yellow]'
                )
                return None

        except (ValueError, KeyboardInterrupt):
            console.print('[yellow]‚ö† L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá, s·∫Ω n·∫°p t·∫•t c·∫£[/yellow]')
            return None

    def _display_loading_results(self):
        """Hi·ªÉn th·ªã k·∫øt qu·∫£ loading"""
        console.print('\n[bold green]‚úÖ ƒê√£ n·∫°p workspace th√†nh c√¥ng![/bold green]')

        # Knowledge statistics
        if self.knowledge_base:
            kb_stats = self._analyze_knowledge_base()

            stats_table = Table(
                title='üìä Th·ªëng k√™ Knowledge Base', title_style='bold green'
            )
            stats_table.add_column('Th√¥ng tin', style='cyan')
            stats_table.add_column('Gi√° tr·ªã', justify='right', style='white')

            stats_table.add_row('üìö T·ªïng t√†i li·ªáu', str(len(self.knowledge_base)))
            stats_table.add_row('üìù Markdown files', str(kb_stats['markdown_count']))
            stats_table.add_row('üêç Python files', str(kb_stats['python_count']))
            stats_table.add_row('üìÅ Folders', str(len(kb_stats['folders'])))
            stats_table.add_row('üîë Keywords', str(kb_stats['total_keywords']))
            stats_table.add_row(
                'üìÑ T·ªïng n·ªôi dung', f'{kb_stats["total_content_length"]:,} k√Ω t·ª±'
            )

            console.print(stats_table)

            # Top folders by content
            if kb_stats['folder_stats']:
                folder_table = Table(
                    title='üìÇ Top Folders theo n·ªôi dung', title_style='bold blue'
                )
                folder_table.add_column('Folder', style='cyan')
                folder_table.add_column('Files', justify='right', style='magenta')
                folder_table.add_column('Content Size', justify='right', style='white')

                for folder, stats in sorted(
                    kb_stats['folder_stats'].items(),
                    key=lambda x: x[1]['content_length'],
                    reverse=True,
                )[:5]:
                    folder_table.add_row(
                        folder,
                        str(stats['file_count']),
                        f'{stats["content_length"]:,} chars',
                    )

                console.print(folder_table)

        # Question statistics
        if self.questions:
            console.print(
                f'[bold yellow]‚ùì ƒê√£ n·∫°p {len(self.questions)} c√¢u h·ªèi t·ª´ {len(self.workspace_structure["question_files"])} files[/bold yellow]'
            )

    def _analyze_knowledge_base(self) -> Dict[str, Any]:
        """Ph√¢n t√≠ch knowledge base ƒë·ªÉ c√≥ th·ªëng k√™"""
        stats = {
            'markdown_count': 0,
            'python_count': 0,
            'folders': set(),
            'total_keywords': 0,
            'total_content_length': 0,
            'folder_stats': {},
        }

        for knowledge in self.knowledge_base:
            # Count by file type
            if knowledge.file_type == 'markdown':
                stats['markdown_count'] += 1
            elif knowledge.file_type == 'python':
                stats['python_count'] += 1

            # Collect folders
            stats['folders'].add(knowledge.folder)

            # Count keywords
            stats['total_keywords'] += len(knowledge.keywords)

            # Count content length
            stats['total_content_length'] += len(knowledge.content)

            # Folder statistics
            folder = knowledge.folder
            if folder not in stats['folder_stats']:
                stats['folder_stats'][folder] = {'file_count': 0, 'content_length': 0}

            stats['folder_stats'][folder]['file_count'] += 1
            stats['folder_stats'][folder]['content_length'] += len(knowledge.content)

        return stats

    def load_data(self, file_paths: List[str]):
        """T·∫£i d·ªØ li·ªáu t·ª´ c√°c file markdown (method c≈©, gi·ªØ ƒë·ªÉ backward compatibility)"""
        parser = MarkdownParser()

        with Progress(
            SpinnerColumn(),
            TextColumn('[progress.description]{task.description}'),
            console=console,
        ) as progress:
            task = progress.add_task('ƒêang t·∫£i d·ªØ li·ªáu...', total=len(file_paths))

            for file_path in file_paths:
                if not os.path.exists(file_path):
                    console.print(f'[red]File kh√¥ng t·ªìn t·∫°i: {file_path}[/red]')
                    continue

                progress.update(task, description=f'ƒêang x·ª≠ l√Ω {Path(file_path).name}')

                if 'question' in file_path.lower():
                    # File ch·ª©a c√¢u h·ªèi
                    questions = parser.parse_questions(file_path)
                    self.questions.extend(questions)
                    console.print(
                        f'[green]‚úì ƒê√£ t·∫£i {len(questions)} c√¢u h·ªèi t·ª´ {Path(file_path).name}[/green]'
                    )
                else:
                    # File ch·ª©a ki·∫øn th·ª©c
                    knowledge = parser.parse_knowledge(file_path)
                    if knowledge:
                        self.knowledge_base.append(knowledge)
                        console.print(
                            f'[green]‚úì ƒê√£ t·∫£i ki·∫øn th·ª©c t·ª´ {Path(file_path).name}[/green]'
                        )

                progress.advance(task)

        console.print(
            f'\n[bold green]T·ªïng c·ªông: {len(self.questions)} c√¢u h·ªèi v√† {len(self.knowledge_base)} t√†i li·ªáu ki·∫øn th·ª©c[/bold green]'
        )

    def start_interview(self):
        """B·∫Øt ƒë·∫ßu ph·ªèng v·∫•n"""
        if not self.questions:
            console.print('[red]Kh√¥ng c√≥ c√¢u h·ªèi n√†o ƒë·ªÉ ph·ªèng v·∫•n![/red]')
            return

        self.session_stats['start_time'] = datetime.now()
        self.session_stats['total_questions'] = len(self.questions)

        console.print(
            Panel.fit(
                '[bold blue]üéØ B·∫Øt ƒë·∫ßu ph·ªèng v·∫•n Hugging Face![/bold blue]\n'
                f'T·ªïng s·ªë c√¢u h·ªèi: {len(self.questions)}\n'
                "G√µ 'help' ƒë·ªÉ xem tr·ª£ gi√∫p, 'quit' ƒë·ªÉ tho√°t",
                title='AI Interview Agent',
                border_style='blue',
            )
        )

        # Shuffle questions for randomness

        random.shuffle(self.questions)

        for i, question in enumerate(self.questions, 1):
            console.print(f'\n[bold cyan]C√¢u h·ªèi {i}/{len(self.questions)}[/bold cyan]')

            if self.ask_question(question):
                self.score += 1
                self.session_stats['correct_answers'] += 1

        self.session_stats['end_time'] = datetime.now()
        self.show_final_results()

    def ask_question(self, question: Question) -> bool:
        """ƒê·∫∑t c√¢u h·ªèi v√† nh·∫≠n ph·∫£n h·ªìi"""
        self.current_question = question

        # Hi·ªÉn th·ªã c√¢u h·ªèi
        console.print(
            Panel(
                f'[bold]{question.title}[/bold]\n\n'
                f'[dim]{question.description}[/dim]\n\n'
                f'[yellow]{question.question}[/yellow]',
                title=f'Question ID: {question.id}',
                border_style='yellow',
            )
        )

        # Hi·ªÉn th·ªã c√°c l·ª±a ch·ªçn
        table = Table(show_header=True, header_style='bold magenta')
        table.add_column('L·ª±a ch·ªçn', style='cyan', width=10)
        table.add_column('N·ªôi dung', style='white')

        for i, option in enumerate(question.options, 1):
            table.add_row(f'({i})', option)

        console.print(table)

        # Nh·∫≠n ph·∫£n h·ªìi t·ª´ user
        while True:
            try:
                response = Prompt.ask(
                    "\n[bold]Ch·ªçn ƒë√°p √°n c·ªßa b·∫°n (1-4) ho·∫∑c g√µ 'help'/'quit'[/bold]",
                    choices=['1', '2', '3', '4', 'help', 'quit', 'h', 'q'],
                    show_choices=False,
                )

                if response.lower() in ['quit', 'q']:
                    console.print('[red]Tho√°t ph·ªèng v·∫•n...[/red]')
                    sys.exit(0)

                if response.lower() in ['help', 'h']:
                    self.show_help()
                    continue

                # Chuy·ªÉn ƒë·ªïi l·ª±a ch·ªçn th√†nh text
                choice_index = int(response) - 1
                if 0 <= choice_index < len(question.options):
                    user_answer = question.options[choice_index]
                    return self.check_answer(user_answer, question)
                else:
                    console.print('[red]L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá![/red]')
                    continue

            except (ValueError, KeyboardInterrupt):
                console.print('[red]L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá![/red]')
                continue

    def check_answer(self, user_answer: str, question: Question) -> bool:
        """Ki·ªÉm tra ƒë√°p √°n v√† ƒë∆∞a ra ph·∫£n h·ªìi"""
        is_correct = user_answer.strip() == question.correct_answer.strip()

        if is_correct:
            console.print(
                Panel(
                    f'[bold green]üéâ Ch√≠nh x√°c![/bold green]\n\n'
                    f'[green]Gi·∫£i th√≠ch:[/green] {question.explanation}',
                    title='K·∫øt qu·∫£',
                    border_style='green',
                )
            )
        else:
            console.print(
                Panel(
                    f'[bold red]‚ùå Sai r·ªìi![/bold red]\n\n'
                    f'[red]ƒê√°p √°n ƒë√∫ng:[/red] {question.correct_answer}\n'
                    f'[red]ƒê√°p √°n c·ªßa b·∫°n:[/red] {user_answer}\n\n'
                    f'[yellow]Gi·∫£i th√≠ch:[/yellow] {question.explanation}',
                    title='K·∫øt qu·∫£',
                    border_style='red',
                )
            )

        # Hi·ªÉn th·ªã ki·∫øn th·ª©c li√™n quan
        self.show_related_knowledge(question)

        # H·ªèi c√≥ mu·ªën ti·∫øp t·ª•c kh√¥ng
        if not Confirm.ask('\n[bold]Ti·∫øp t·ª•c c√¢u h·ªèi ti·∫øp theo?[/bold]', default=True):
            console.print('[yellow]K·∫øt th√∫c ph·ªèng v·∫•n...[/yellow]')
            self.show_final_results()
            sys.exit(0)

        return is_correct

    def show_related_knowledge(self, question: Question):
        """Hi·ªÉn th·ªã ki·∫øn th·ª©c li√™n quan ƒë·∫øn c√¢u h·ªèi"""
        # T√¨m ki·∫øn th·ª©c li√™n quan d·ª±a tr√™n t·ª´ kh√≥a
        related_knowledge = []
        question_keywords = (
            question.title.lower().split() + question.question.lower().split()
        )

        for knowledge in self.knowledge_base:
            for keyword in question_keywords:
                if (
                    keyword in knowledge.keywords
                    or keyword in knowledge.content.lower()
                ):
                    related_knowledge.append(knowledge)
                    break

        if related_knowledge:
            console.print('\n[bold blue]üìö Ki·∫øn th·ª©c li√™n quan:[/bold blue]')
            for i, knowledge in enumerate(
                related_knowledge[:2], 1
            ):  # Ch·ªâ hi·ªÉn th·ªã 2 t√†i li·ªáu ƒë·∫ßu
                console.print(
                    f'[dim]{i}. {knowledge.title} (t·ª´ {Path(knowledge.source_file).name})[/dim]'
                )

    def show_help(self):
        """Hi·ªÉn th·ªã tr·ª£ gi√∫p"""
        help_text = """
        [bold blue]üîß H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:[/bold blue]

        [yellow]C√°c l·ªánh c√≥ s·∫µn:[/yellow]
        ‚Ä¢ 1-4: Ch·ªçn ƒë√°p √°n
        ‚Ä¢ help (h): Hi·ªÉn th·ªã tr·ª£ gi√∫p n√†y
        ‚Ä¢ quit (q): Tho√°t ch∆∞∆°ng tr√¨nh

        [yellow]C√°ch th·ª©c ho·∫°t ƒë·ªông:[/yellow]
        ‚Ä¢ Ch·ªçn s·ªë t∆∞∆°ng ·ª©ng v·ªõi ƒë√°p √°n b·∫°n cho l√† ƒë√∫ng
        ‚Ä¢ H·ªá th·ªëng s·∫Ω ki·ªÉm tra v√† ƒë∆∞a ra gi·∫£i th√≠ch
        ‚Ä¢ ƒêi·ªÉm s·ªë v√† th·ªëng k√™ s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã cu·ªëi phi√™n

        [yellow]M·∫πo:[/yellow]
        ‚Ä¢ ƒê·ªçc k·ªπ m√¥ t·∫£ c√¢u h·ªèi tr∆∞·ªõc khi ch·ªçn
        ‚Ä¢ Ch√∫ √Ω ƒë·∫øn t·ª´ kh√≥a quan tr·ªçng
        ‚Ä¢ S·ª≠ d·ª•ng ki·∫øn th·ª©c t·ª´ t√†i li·ªáu ƒë√£ t·∫£i
        """
        console.print(Panel(help_text, title='Tr·ª£ gi√∫p', border_style='blue'))

    def show_final_results(self):
        """Hi·ªÉn th·ªã k·∫øt qu·∫£ cu·ªëi c√πng"""
        if self.session_stats['start_time'] and self.session_stats['end_time']:
            duration = self.session_stats['end_time'] - self.session_stats['start_time']
            duration_str = f'{duration.total_seconds():.1f} gi√¢y'
        else:
            duration_str = 'N/A'

        total = self.session_stats['total_questions']
        correct = self.session_stats['correct_answers']
        percentage = (correct / total * 100) if total > 0 else 0

        # ƒê√°nh gi√° k·∫øt qu·∫£
        if percentage >= 80:
            grade = 'Xu·∫•t s·∫Øc! üåü'
            color = 'green'
        elif percentage >= 60:
            grade = 'Kh√° t·ªët! üëç'
            color = 'yellow'
        elif percentage >= 40:
            grade = 'C·∫ßn c·∫£i thi·ªán üìö'
            color = 'orange'
        else:
            grade = 'C·∫ßn h·ªçc th√™m üí™'
            color = 'red'

        result_table = Table(title='üéØ K·∫øt qu·∫£ ph·ªèng v·∫•n', title_style='bold blue')
        result_table.add_column('Ch·ªâ s·ªë', style='cyan', width=20)
        result_table.add_column('Gi√° tr·ªã', style='white')

        result_table.add_row('T·ªïng c√¢u h·ªèi', str(total))
        result_table.add_row('C√¢u tr·∫£ l·ªùi ƒë√∫ng', str(correct))
        result_table.add_row('T·ª∑ l·ªá ch√≠nh x√°c', f'{percentage:.1f}%')
        result_table.add_row('Th·ªùi gian', duration_str)
        result_table.add_row('ƒê√°nh gi√°', f'[{color}]{grade}[/{color}]')

        console.print('\n')
        console.print(result_table)

        # L·ªùi khuy√™n
        console.print(
            Panel(
                self.get_advice(percentage), title='üí° L·ªùi khuy√™n', border_style='blue'
            )
        )

    def get_advice(self, percentage: float) -> str:
        """ƒê∆∞a ra l·ªùi khuy√™n d·ª±a tr√™n k·∫øt qu·∫£"""
        if percentage >= 80:
            return '[green]B·∫°n ƒë√£ c√≥ ki·∫øn th·ª©c r·∫•t t·ªët v·ªÅ Hugging Face! Ti·∫øp t·ª•c duy tr√¨ v√† kh√°m ph√° c√°c t√≠nh nƒÉng n√¢ng cao.[/green]'
        elif percentage >= 60:
            return '[yellow]B·∫°n ƒë√£ n·∫Øm ƒë∆∞·ª£c nh·ªØng ki·∫øn th·ª©c c∆° b·∫£n. H√£y t·∫≠p trung v√†o nh·ªØng ph·∫ßn c√≤n thi·∫øu v√† th·ª±c h√†nh th√™m.[/yellow]'
        elif percentage >= 40:
            return '[orange]B·∫°n c·∫ßn ƒë·ªçc l·∫°i t√†i li·ªáu v√† th·ª±c h√†nh th√™m. T·∫≠p trung v√†o c√°c kh√°i ni·ªám c∆° b·∫£n tr∆∞·ªõc.[/orange]'
        else:
            return '[red]H√£y b·∫Øt ƒë·∫ßu t·ª´ nh·ªØng ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ Hugging Face. ƒê·ªçc documentation v√† l√†m theo tutorial.[/red]'


class ChatMode:
    """Ch·∫ø ƒë·ªô chat t∆∞∆°ng t√°c v·ªõi AI Ollama"""

    def __init__(self, agent: InterviewAgent):
        self.agent = agent
        self.conversation_history = []
        self.ollama_ai = OllamaAI()
        self.use_ai = self.ollama_ai.is_available

    def start_chat(self):
        """B·∫Øt ƒë·∫ßu ch·∫ø ƒë·ªô chat"""
        ai_status = 'ü§ñ AI Ollama' if self.use_ai else 'üìö Rule-based'
        console.print(
            Panel.fit(
                f'[bold green]üí¨ Ch·∫ø ƒë·ªô Chat t∆∞∆°ng t√°c ({ai_status})[/bold green]\n'
                'H·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ Hugging Face!\n'
                "G√µ 'quit' ƒë·ªÉ tho√°t, 'interview' ƒë·ªÉ chuy·ªÉn sang ch·∫ø ƒë·ªô ph·ªèng v·∫•n\n"
                f"G√µ 'ai' ƒë·ªÉ {'t·∫Øt' if self.use_ai else 'b·∫≠t'} AI mode",
                title='AI Chat Assistant',
                border_style='green',
            )
        )

        while True:
            try:
                user_input = Prompt.ask('\n[bold blue]B·∫°n[/bold blue]')

                if user_input.lower() in ['quit', 'q', 'exit']:
                    console.print('[green]T·∫°m bi·ªát! üëã[/green]')
                    break

                if user_input.lower() in ['interview', 'test']:
                    console.print('[yellow]Chuy·ªÉn sang ch·∫ø ƒë·ªô ph·ªèng v·∫•n...[/yellow]')
                    self.agent.start_interview()
                    continue

                if user_input.lower() == 'ai':
                    self.use_ai = not self.use_ai
                    status = 'ü§ñ AI Ollama' if self.use_ai else 'üìö Rule-based'
                    console.print(f'[yellow]Chuy·ªÉn sang ch·∫ø ƒë·ªô: {status}[/yellow]')
                    continue

                # X·ª≠ l√Ω c√¢u h·ªèi
                if self.use_ai:
                    response = self.process_question_with_ai(user_input)
                else:
                    response = self.process_question_rule_based(user_input)

                console.print('\n[bold green]ü§ñ AI Assistant[/bold green]')
                console.print(Panel(response, border_style='green'))

            except KeyboardInterrupt:
                console.print('\n[green]T·∫°m bi·ªát! üëã[/green]')
                break

    def _build_context(self, question: str) -> str:
        """X√¢y d·ª±ng context t·ª´ knowledge base cho AI"""
        question_lower = question.lower()
        relevant_knowledge = []

        # T√¨m ki·∫øn th·ª©c li√™n quan
        for knowledge in self.agent.knowledge_base:
            relevance_score = 0

            # Ki·ªÉm tra keywords
            for keyword in knowledge.keywords:
                if keyword in question_lower:
                    relevance_score += 2

            # Ki·ªÉm tra content
            content_lower = knowledge.content.lower()
            for word in question_lower.split():
                if word in content_lower:
                    relevance_score += 1

            if relevance_score > 0:
                relevant_knowledge.append((relevance_score, knowledge))

        # S·∫Øp x·∫øp theo relevance score
        relevant_knowledge.sort(key=lambda x: x[0], reverse=True)

        # T·∫°o context
        context = ''
        for _score, knowledge in relevant_knowledge[:3]:  # L·∫•y top 3
            context += f'## {knowledge.title}\n'
            context += f'Source: {Path(knowledge.source_file).name}\n'
            context += f'Content: {knowledge.content[:800]}...\n\n'

        return (
            context if context else 'Kh√¥ng c√≥ th√¥ng tin li√™n quan trong knowledge base.'
        )

    def process_question_with_ai(self, question: str) -> str:
        """X·ª≠ l√Ω c√¢u h·ªèi v·ªõi AI Ollama"""
        try:
            # T·∫°o context t·ª´ knowledge base
            context = self._build_context(question)

            # G·ªçi AI
            with console.status('[bold green]ü§ñ AI ƒëang suy nghƒ©...', spinner='dots'):
                ai_response = self.ollama_ai.generate_response(question, context)

            # Format response v·ªõi template ƒë·∫πp
            return self._format_ai_response(ai_response, question)

        except Exception as e:
            console.print(f'[red]L·ªói AI: {e}[/red]')
            return self.process_question_rule_based(question)

    def _format_ai_response(self, ai_response: AIResponse, question: str) -> str:
        """Format AI response v·ªõi template ƒë·∫πp v√† clean"""
        lines = []

        # Header v·ªõi confidence
        confidence_emoji = self._get_confidence_emoji(ai_response.confidence)
        lines.append(
            f'ü§ñ AI Analysis {confidence_emoji} ({ai_response.confidence:.0%} confidence)'
        )
        lines.append('')

        # Main content - l√†m s·∫°ch v√† format
        clean_content = self._clean_ai_content(ai_response.content)
        if clean_content:
            lines.append('üìã Answer:')
            lines.append('‚îÄ' * 50)
            lines.extend(self._format_content_lines(clean_content))
            lines.append('')

        # Thinking process (n·∫øu c√≥) - compact format
        if ai_response.thinking_process:
            thinking_clean = self._clean_ai_content(ai_response.thinking_process)
            if thinking_clean and len(thinking_clean) > 20:  # Ch·ªâ hi·ªán n·∫øu c√≥ n·ªôi dung
                lines.append('üí≠ AI Reasoning:')
                lines.append('‚îÄ' * 30)
                # R√∫t g·ªçn thinking process
                thinking_summary = self._summarize_thinking(thinking_clean)
                lines.append(f'   {thinking_summary}')
                lines.append('')

        # Footer v·ªõi source info
        if ai_response.source == 'ollama':
            lines.append('üîç Analysis based on:')
            lines.append(
                f'   ‚Ä¢ Knowledge Base: {len(self.agent.knowledge_base)} documents'
            )
            lines.append('   ‚Ä¢ AI Model: Ollama Llama3')

        return '\n'.join(lines)

    def _get_confidence_emoji(self, confidence: float) -> str:
        """L·∫•y emoji ph√π h·ª£p v·ªõi confidence level"""
        if confidence >= 0.8:
            return 'üéØ'  # High confidence
        elif confidence >= 0.6:
            return '‚úÖ'  # Good confidence
        elif confidence >= 0.4:
            return '‚ö†Ô∏è'  # Medium confidence
        else:
            return '‚ùì'  # Low confidence

    def _clean_ai_content(self, content: str) -> str:
        """L√†m s·∫°ch content AI, lo·∫°i b·ªè markdown v√† formatting kh√¥ng c·∫ßn thi·∫øt"""
        if not content:
            return ''

        # Lo·∫°i b·ªè markdown formatting
        content = re.sub(r'\*\*(.*?)\*\*', r'\1', content)  # **bold** -> bold
        content = re.sub(r'\*(.*?)\*', r'\1', content)  # *italic* -> italic
        content = re.sub(r'`(.*?)`', r'\1', content)  # `code` -> code
        content = re.sub(r'#{1,6}\s*', '', content)  # headings
        content = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', content)  # links

        # L√†m s·∫°ch c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát - C·∫®N TH·∫¨N KH√îNG L·∫∂P
        # Ch·ªâ chu·∫©n h√≥a c√°c d·∫•u bullet ·ªü ƒë·∫ßu d√≤ng
        content = re.sub(r'^[\s]*[-\*]\s+', '‚Ä¢ ', content, flags=re.MULTILINE)
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)  # Lo·∫°i b·ªè line breaks th·ª´a

        # Lo·∫°i b·ªè c√°c th·∫ª thinking
        content = re.sub(r'<thinking>.*?</thinking>', '', content, flags=re.DOTALL)

        return content.strip()

    def _format_content_lines(self, content: str) -> List[str]:
        """Format content th√†nh c√°c d√≤ng ƒë·∫πp v·ªõi indentation"""
        lines = content.split('\n')
        formatted = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Bullet points
            if line.startswith('‚Ä¢'):
                formatted.append(f'   {line}')
            # Numbered lists
            elif re.match(r'^\d+\.', line):
                formatted.append(f'   {line}')
            # Headers or important lines
            elif line.isupper() or line.endswith(':'):
                formatted.append(f'\n   {line}')
            # Regular content
            else:
                # Wrap long lines
                if len(line) > 80:
                    wrapped = self._wrap_text(line, 80)
                    for wrapped_line in wrapped:
                        formatted.append(f'   {wrapped_line}')
                else:
                    formatted.append(f'   {line}')

        return formatted

    def _wrap_text(self, text: str, width: int) -> List[str]:
        """Wrap text ƒë·ªÉ kh√¥ng qu√° d√†i"""
        words = text.split()
        lines = []
        current_line = []
        current_length = 0

        for word in words:
            if current_length + len(word) + 1 <= width:
                current_line.append(word)
                current_length += len(word) + 1
            else:
                if current_line:
                    lines.append(' '.join(current_line))
                current_line = [word]
                current_length = len(word)

        if current_line:
            lines.append(' '.join(current_line))

        return lines

    def _summarize_thinking(self, thinking: str) -> str:
        """T√≥m t·∫Øt thinking process ƒë·ªÉ hi·ªÉn th·ªã g·ªçn"""
        # L·∫•y c√¢u ƒë·∫ßu ti√™n ho·∫∑c 100 k√Ω t·ª± ƒë·∫ßu
        sentences = thinking.split('.')
        if sentences and len(sentences[0]) > 20:
            summary = sentences[0].strip()
            if len(summary) > 100:
                summary = summary[:100] + '...'
            return summary
        else:
            return thinking[:100] + '...' if len(thinking) > 100 else thinking

    def process_question_rule_based(self, question: str) -> str:
        """X·ª≠ l√Ω c√¢u h·ªèi theo rule-based (ph∆∞∆°ng ph√°p c≈©)"""
        return self.process_question(question)

    def process_question(self, question: str) -> str:
        """X·ª≠ l√Ω c√¢u h·ªèi v√† tr·∫£ l·ªùi d·ª±a tr√™n knowledge base"""
        question_lower = question.lower()

        # T√¨m ki·∫øn th·ª©c li√™n quan
        relevant_knowledge = []
        for knowledge in self.agent.knowledge_base:
            for keyword in knowledge.keywords:
                if keyword in question_lower:
                    relevant_knowledge.append(knowledge)
                    break

        # N·∫øu kh√¥ng t√¨m th·∫•y ki·∫øn th·ª©c li√™n quan, t√¨m theo content
        if not relevant_knowledge:
            for knowledge in self.agent.knowledge_base:
                if any(
                    word in knowledge.content.lower() for word in question_lower.split()
                ):
                    relevant_knowledge.append(knowledge)

        if relevant_knowledge:
            return self._format_rule_based_response(relevant_knowledge, question)
        else:
            return self._format_no_results_response()

    def _format_rule_based_response(
        self, relevant_knowledge: List[Knowledge], question: str
    ) -> str:
        """Format rule-based response v·ªõi template ƒë·∫πp"""
        lines = []

        # Header
        lines.append('üìö Knowledge Base Search ‚úÖ')
        lines.append('')

        # Main content
        lines.append('üìã Found Information:')
        lines.append('‚îÄ' * 50)

        for i, knowledge in enumerate(
            relevant_knowledge[:2], 1
        ):  # Ch·ªâ l·∫•y 2 t√†i li·ªáu ƒë·∫ßu
            lines.append(f'\nÔøΩ Source {i}: {self._clean_markdown(knowledge.title)}')
            lines.append('‚îÄ' * 30)

            # S·ª≠ d·ª•ng smart search ƒë·ªÉ t√¨m n·ªôi dung li√™n quan
            smart_results = self._smart_search(question, knowledge)

            if smart_results:
                # Hi·ªÉn th·ªã k·∫øt qu·∫£ t√¨m ki·∫øm th√¥ng minh
                for result in smart_results[:3]:  # Gi·ªõi h·∫°n 3 k·∫øt qu·∫£
                    cleaned_result = self._clean_markdown(result)
                    if cleaned_result and len(cleaned_result) > 10:
                        # Format v·ªõi indentation - lo·∫°i b·ªè bullet c√≥ s·∫µn
                        formatted_result = self._format_single_result(cleaned_result)
                        # Lo·∫°i b·ªè bullet ·ªü ƒë·∫ßu n·∫øu c√≥
                        if formatted_result.startswith('‚Ä¢ '):
                            formatted_result = formatted_result[2:]
                        lines.append(f'   ‚Ä¢ {formatted_result}')
            else:
                # Fallback: extract key points
                key_points = self._extract_key_points(knowledge.content)
                if key_points:
                    for point in key_points[:3]:  # Gi·ªõi h·∫°n 3 ƒëi·ªÉm
                        lines.append(f'   ‚Ä¢ {point}')
                else:
                    # Final fallback: clean content summary
                    summary = self._create_summary(knowledge.content)
                    lines.append(f'   {summary}')

            lines.append(f'\n   üìÅ From: {Path(knowledge.source_file).name}')

        # Footer
        lines.append('')
        lines.append('üîç Search based on:')
        lines.append('   ‚Ä¢ Keyword matching and content analysis')
        lines.append(f'   ‚Ä¢ Knowledge Base: {len(self.agent.knowledge_base)} documents')

        return '\n'.join(lines)

    def _format_no_results_response(self) -> str:
        """Format response khi kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£"""
        lines = []

        lines.append('üìö Knowledge Base Search ‚ùå')
        lines.append('')
        lines.append('üìã No Direct Match Found')
        lines.append('‚îÄ' * 50)
        lines.append('')
        lines.append('üí° Suggestions:')
        lines.append('   ‚Ä¢ Try asking about: Hugging Face, Models, Hub, API')
        lines.append("   ‚Ä¢ Switch to interview mode: type 'interview'")
        lines.append("   ‚Ä¢ Toggle AI mode: type 'ai'")
        lines.append('')
        lines.append('üîç Available Resources:')
        lines.append(f'   ‚Ä¢ Knowledge Base: {len(self.agent.knowledge_base)} documents')
        lines.append(f'   ‚Ä¢ Question Bank: {len(self.agent.questions)} questions')

        return '\n'.join(lines)

    def _format_single_result(self, result: str) -> str:
        """Format m·ªôt k·∫øt qu·∫£ t√¨m ki·∫øm"""
        # Lo·∫°i b·ªè k√Ω t·ª± th·ª´a v√† format ƒë·∫πp
        result = result.strip()

        # N·∫øu qu√° d√†i, c·∫Øt ng·∫Øn
        if len(result) > 120:
            result = result[:120] + '...'

        return result

    def _create_summary(self, content: str) -> str:
        """T·∫°o summary ng·∫Øn g·ªçn t·ª´ content"""
        # L·∫•y c√¢u ƒë·∫ßu ti√™n ho·∫∑c ƒëo·∫°n ƒë·∫ßu
        sentences = content.split('.')
        if sentences and len(sentences[0].strip()) > 20:
            summary = sentences[0].strip()
            if len(summary) > 150:
                summary = summary[:150] + '...'
            return summary
        else:
            # Fallback: l·∫•y ƒëo·∫°n ƒë·∫ßu
            paragraphs = content.split('\n\n')
            if paragraphs:
                first_para = paragraphs[0].strip()
                if len(first_para) > 200:
                    first_para = first_para[:200] + '...'
                return self._clean_markdown(first_para)

        return 'Content available but requires specific keywords to search.'

    def _clean_markdown(self, text: str) -> str:
        """Lo·∫°i b·ªè c√°c k√Ω t·ª± markdown formatting"""
        if not text:
            return ''

        # Lo·∫°i b·ªè markdown formatting
        text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # **bold** -> bold
        text = re.sub(r'\*(.*?)\*', r'\1', text)  # *italic* -> italic
        text = re.sub(r'`(.*?)`', r'\1', text)  # `code` -> code
        text = re.sub(r'#{1,6}\s*', '', text)  # ## heading -> heading
        text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)  # [text](url) -> text
        text = re.sub(
            r'^\s*[-*+]\s*', '‚Ä¢ ', text, flags=re.MULTILINE
        )  # - item -> ‚Ä¢ item

        return text.strip()

    def _format_content(self, content: str) -> str:
        """Format n·ªôi dung ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n"""
        lines = content.split('\n')
        formatted_lines = []

        for line in lines:
            line = line.strip()
            if line:
                if line.startswith('‚Ä¢'):
                    formatted_lines.append(line)
                else:
                    formatted_lines.append(f'{line}')

        return '\n'.join(formatted_lines[:4])  # Gi·ªõi h·∫°n 4 d√≤ng

    def _smart_search(self, question: str, knowledge: Knowledge) -> List[str]:
        """T√¨m ki·∫øm th√¥ng minh trong knowledge base"""
        question_words = set(question.lower().split())
        lines = knowledge.content.split('\n')
        scored_lines = []

        for line in lines:
            line = line.strip()
            if not line or line.startswith('#'):
                continue

            # T√≠nh ƒëi·ªÉm relevance
            line_words = set(line.lower().split())
            score = len(question_words.intersection(line_words))

            # Bonus ƒëi·ªÉm cho d√≤ng ch·ª©a t·ª´ kh√≥a quan tr·ªçng
            if any(
                keyword in line.lower()
                for keyword in ['hugging face', 'model', 'api', 'hub']
            ):
                score += 2

            if score > 0:
                scored_lines.append((score, line))

        # Sort theo ƒëi·ªÉm v√† l·∫•y top results
        scored_lines.sort(key=lambda x: x[0], reverse=True)
        return [line for score, line in scored_lines[:5]]

    def _extract_key_points(self, content: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c ƒëi·ªÉm ch√≠nh t·ª´ n·ªôi dung"""
        lines = content.split('\n')
        key_points = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # T√¨m c√°c bullet points
            if line.startswith(('- ', '‚Ä¢ ', '* ')):
                cleaned = self._clean_markdown(line[2:].strip())
                if cleaned:
                    key_points.append(cleaned)

            # T√¨m c√°c heading quan tr·ªçng
            elif line.startswith('###'):
                cleaned = self._clean_markdown(line[3:].strip())
                if cleaned:
                    key_points.append(f'üìå {cleaned}')

        return key_points[:6]  # Gi·ªõi h·∫°n 6 ƒëi·ªÉm ch√≠nh


@click.command()
@click.argument('files', nargs=-1, type=click.Path(exists=True))
@click.option(
    '--mode',
    '-m',
    type=click.Choice(['interview', 'chat', 'both']),
    default='both',
    help='Ch·ªçn ch·∫ø ƒë·ªô: interview (ph·ªèng v·∫•n), chat (tr√≤ chuy·ªán), both (c·∫£ hai)',
)
@click.option('--shuffle', '-s', is_flag=True, help='X√°o tr·ªôn th·ª© t·ª± c√¢u h·ªèi')
@click.option('--limit', '-l', type=int, help='Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng c√¢u h·ªèi')
@click.option('--verbose', '-v', is_flag=True, help='Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt')
@click.option(
    '--workspace',
    '-w',
    type=click.Path(exists=True),
    help='ƒê∆∞·ªùng d·∫´n workspace ƒë·ªÉ load to√†n b·ªô',
)
@click.option(
    '--folders',
    '-f',
    type=str,
    help='Danh s√°ch folders c√°ch nhau b·ªüi d·∫•u ph·∫©y (vd: getting-started,pipelines)',
)
@click.option('--include-python', is_flag=True, help='Bao g·ªìm ph√¢n t√≠ch file Python')
@click.option(
    '--max-file-size',
    type=int,
    default=1024 * 1024,
    help='K√≠ch th∆∞·ªõc file t·ªëi ƒëa (bytes)',
)
@click.option(
    '--exclude-folders',
    type=str,
    help='Lo·∫°i tr·ª´ folders (vd: __pycache__,node_modules)',
)
def main(
    files,
    mode,
    shuffle,
    limit,
    verbose,
    workspace,
    folders,
    include_python,
    max_file_size,
    exclude_folders,
):
    """
    AI Interview Agent - Hugging Face Knowledge Assessment

    Ph√¢n t√≠ch c√°c file .md v√† t·∫°o phi√™n ph·ªèng v·∫•n t∆∞∆°ng t√°c.

    Examples:
        # Traditional file-based loading
        python main.py getting-started/questions.md getting-started/introduction.md
        python main.py *.md --mode chat

        # New workspace loading
        python main.py --workspace . --mode chat
        python main.py --workspace . --folders 'getting-started,pipelines'
        python main.py --workspace . --include-python --exclude-folders '__pycache__'
    """

    # Banner
    console.print(
        Panel.fit(
            '[bold blue]ü§ñ AI Interview Agent[/bold blue]\n'
            '[dim]Hugging Face Knowledge Assessment[/dim]\n'
            f'[green]Mode: {mode.upper()}[/green]',
            title='Welcome',
            border_style='blue',
        )
    )

    # Kh·ªüi t·∫°o agent
    agent = InterviewAgent()

    # Configure workspace loading if enabled
    if workspace or folders or include_python:
        # Configure workspace loader
        config = WorkspaceConfig()
        config.enable_code_analysis = include_python
        config.max_file_size = max_file_size

        if exclude_folders:
            config.exclude_folders = exclude_folders.split(',')

        agent.workspace_loader.config = config

        # Determine workspace path
        workspace_path = workspace or '.'

        # Parse folders if specified
        selected_folders = folders.split(',') if folders else None

        # Load workspace content
        console.print(f'\n[bold]üåê Loading workspace: {workspace_path}[/bold]')
        if selected_folders:
            console.print(
                f'[yellow]üìÅ Selected folders: {", ".join(selected_folders)}[/yellow]'
            )

        knowledge_base = agent.workspace_loader.load_workspace_content(
            workspace_path, selected_folders or []
        )

        # Set knowledge base and load questions
        agent.knowledge_base = knowledge_base
        agent.questions = []

        # Try to load questions from workspace structure
        workspace_structure = agent.workspace_loader.discover_workspace(workspace_path)
        parser = MarkdownParser()

        for question_file in workspace_structure['question_files']:
            questions = parser.parse_questions(question_file)
            agent.questions.extend(questions)

        console.print(
            f'[green]‚úì Loaded {len(knowledge_base)} documents and {len(agent.questions)} questions[/green]'
        )

    elif files:
        # Traditional file-based loading
        console.print(f'\n[bold]üìÅ Loading {len(files)} file(s)...[/bold]')
        agent.load_data(list(files))

        if verbose:
            console.print(
                f'\n[dim]Loaded {len(agent.questions)} questions and {len(agent.knowledge_base)} documents[/dim]'
            )

    else:
        # Auto-discovery mode
        console.print('\n[yellow]üîç Auto-discovery mode[/yellow]')
        console.print('[dim]Looking for markdown files in current directory...[/dim]')

        workspace_structure = agent.workspace_loader.discover_workspace('.')
        markdown_files = workspace_structure['markdown_files']

        if markdown_files:
            console.print(f'[green]Found {len(markdown_files)} markdown files[/green]')

            # Load the discovered files
            agent.load_data(markdown_files[:10])  # Limit to first 10 files
            console.print(f'[green]‚úì Loaded {len(agent.questions)} questions[/green]')
        else:
            console.print('[red]‚ùå No markdown files found![/red]')
            console.print('\n[yellow]Examples:[/yellow]')
            console.print('  python main.py getting-started/questions.md')
            console.print('  python main.py --workspace . --mode chat')
            return

    # Apply limit
    if limit and limit < len(agent.questions):
        agent.questions = agent.questions[:limit]
        console.print(f'[yellow]Question limit applied: {limit}[/yellow]')

    # Apply shuffle
    if shuffle and agent.questions:
        import random

        random.shuffle(agent.questions)
        console.print('[yellow]Questions shuffled[/yellow]')

    # Ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông
    if mode == 'interview':
        agent.start_interview()
    elif mode == 'chat':
        chat = ChatMode(agent)
        chat.start_chat()
    else:  # both
        console.print('\n[bold yellow]Ch·ªçn ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông:[/bold yellow]')
        console.print('1. üéØ Ph·ªèng v·∫•n (Interview)')
        console.print('2. üí¨ Tr√≤ chuy·ªán (Chat)')
        console.print('3. üîÑ C·∫£ hai (Both)')

        choice = Prompt.ask('Ch·ªçn ch·∫ø ƒë·ªô', choices=['1', '2', '3'], default='1')

        if choice == '1':
            agent.start_interview()
        elif choice == '2':
            chat = ChatMode(agent)
            chat.start_chat()
        else:
            chat = ChatMode(agent)
            console.print(
                "\n[bold]B·∫Øt ƒë·∫ßu v·ªõi ch·∫ø ƒë·ªô chat, g√µ 'interview' ƒë·ªÉ chuy·ªÉn sang ph·ªèng v·∫•n[/bold]"
            )
            chat.start_chat()


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        console.print('\n[green]T·∫°m bi·ªát! üëã[/green]')
    except Exception as e:
        console.print(f'\n[red]L·ªói: {e}[/red]')
        if '--verbose' in sys.argv:
            import traceback

            console.print(traceback.format_exc())
