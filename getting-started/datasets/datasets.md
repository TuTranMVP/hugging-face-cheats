# Hugging Face Datasets

## ğŸ“š Tá»•ng quan

Hugging Face Hub cung cáº¥p hÃ ng nghÃ¬n datasets Ä‘Æ°á»£c cá»™ng Ä‘á»“ng tuyá»ƒn chá»n cho nhiá»u tÃ¡c vá»¥ vÃ  lÄ©nh vá»±c khÃ¡c nhau. ThÆ° viá»‡n **datasets** giÃºp truy cáº­p, táº£i xuá»‘ng, xá»­ lÃ½ vÃ  chia sáº» dá»¯ liá»‡u chá»‰ vá»›i vÃ i dÃ²ng code.

## ğŸ” KhÃ¡m phÃ¡ Datasets

### Truy cáº­p Hub
- **Datasets Hub**: [huggingface.co/datasets](https://huggingface.co/datasets)
- **Documentation**: [huggingface.co/docs/datasets](https://huggingface.co/docs/datasets/index)

### TÃ­nh nÄƒng tÃ¬m kiáº¿m
- **Lá»c theo task**: Text classification, sentiment analysis, translation...
- **Lá»c theo ngÃ´n ngá»¯**: Tiáº¿ng Viá»‡t, Tiáº¿ng Anh, Ä‘a ngÃ´n ngá»¯...
- **Lá»c theo kÃ­ch thÆ°á»›c**: Nhá» (<1MB), trung bÃ¬nh (1MB-1GB), lá»›n (>1GB)
- **Lá»c theo license**: MIT, Apache, Creative Commons...

### Dataset Cards
Má»—i dataset cÃ³ thÃ´ng tin chi tiáº¿t:
- âœ… **Dataset path**: Äá»‹nh danh duy nháº¥t (vd: `imdb`, `squad`)
- âœ… **Description**: MÃ´ táº£ má»¥c Ä‘Ã­ch vÃ  ná»™i dung
- âœ… **Dataset structure**: Cáº¥u trÃºc cá»™t vÃ  kiá»ƒu dá»¯ liá»‡u
- âœ… **Examples**: Máº«u dá»¯ liá»‡u thá»±c táº¿
- âœ… **Field metadata**: ThÃ´ng tin vá» tá»«ng trÆ°á»ng
- âœ… **Viewer**: Hiá»ƒn thá»‹ 20 dÃ²ng Ä‘áº§u tiÃªn

## ğŸ“¦ CÃ i Ä‘áº·t vÃ  Import

```bash
pip install datasets
```

```python
from datasets import load_dataset, load_dataset_builder
```

## ğŸ” Kiá»ƒm tra Dataset trÆ°á»›c khi táº£i

### Inspect metadata
```python
# Kiá»ƒm tra thÃ´ng tin dataset trÆ°á»›c khi download
builder = load_dataset_builder("imdb")

# Kiá»ƒm tra kÃ­ch thÆ°á»›c dataset
dataset_size_mb = builder.info.dataset_size / (1024 * 1024)
print(f"Dataset size: {dataset_size_mb:.1f} MB")

# Kiá»ƒm tra cáº¥u trÃºc
print("Features:", builder.info.features)
print("Splits:", list(builder.info.splits.keys()))
```

### ThÃ´ng tin quan trá»ng cáº§n check:
- ğŸ“Š **Dataset size**: Äáº£m báº£o Ä‘á»§ storage vÃ  bandwidth
- ğŸ—‚ï¸ **Available splits**: train, test, validation
- ğŸ—ï¸ **Data structure**: CÃ¡c columns vÃ  data types
- ğŸ“ **Description**: Hiá»ƒu rÃµ má»¥c Ä‘Ã­ch sá»­ dá»¥ng

## ğŸ“¥ Táº£i Dataset

### Táº£i toÃ n bá»™ dataset
```python
# Táº£i táº¥t cáº£ splits
dataset = load_dataset("imdb")
print(dataset)

# Káº¿t quáº£:
# DatasetDict({
#     train: Dataset({features: ['text', 'label'], num_rows: 25000})
#     test: Dataset({features: ['text', 'label'], num_rows: 25000})
# })
```

### Táº£i split cá»¥ thá»ƒ
```python
# Chá»‰ táº£i train split
train_dataset = load_dataset("imdb", split="train")
print(f"Train samples: {len(train_dataset)}")

# Táº£i nhiá»u splits
test_dataset = load_dataset("imdb", split="test")
```

### Táº£i subset nhá» Ä‘á»ƒ test
```python
# Táº£i chá»‰ 100 samples Ä‘áº§u tiÃªn
small_dataset = load_dataset("imdb", split="train[:100]")
print(f"Small dataset: {len(small_dataset)} samples")
```

## ğŸ› ï¸ Xá»­ lÃ½ vÃ  Thao tÃ¡c Dataset

### Apache Arrow Format
- ğŸ“Š **Columnar storage**: LÆ°u trá»¯ theo cá»™t thay vÃ¬ hÃ ng
- âš¡ **Hiá»‡u suáº¥t cao**: Truy váº¥n vÃ  xá»­ lÃ½ nhanh hÆ¡n
- ğŸ’¾ **Memory efficient**: Tá»‘i Æ°u sá»­ dá»¥ng RAM

### Filtering Data
```python
# Filter theo Ä‘iá»u kiá»‡n
positive_reviews = dataset["train"].filter(lambda x: x["label"] == 1)
print(f"Positive reviews: {len(positive_reviews)}")

# Filter phá»©c táº¡p hÆ¡n
long_texts = dataset["train"].filter(lambda x: len(x["text"]) > 500)
```

### Selecting Data
```python
# Láº¥y 2 rows Ä‘áº§u tiÃªn
first_two = dataset["train"].select(range(2))
print(f"Selected: {len(first_two)} rows")

# Truy cáº­p dá»¯ liá»‡u cá»¥ thá»ƒ
print("First text:", first_two[0]["text"])
print("First label:", first_two[0]["label"])
```

### Map vÃ  Transform
```python
# Ãp dá»¥ng function cho táº¥t cáº£ rows
def preprocess_text(examples):
    return {"text": [text.lower().strip() for text in examples["text"]]}

processed_dataset = dataset["train"].map(preprocess_text, batched=True)
```

## ğŸ’¡ Best Practices cho Developer Viá»‡t Nam

### 1. Kiá»ƒm tra trÆ°á»›c khi táº£i
```python
# LuÃ´n check size trÆ°á»›c khi download
builder = load_dataset_builder("dataset_name")
size_gb = builder.info.dataset_size / (1024**3)
if size_gb > 5:  # Náº¿u > 5GB
    print(f"âš ï¸ Large dataset: {size_gb:.1f} GB")
    confirm = input("Continue? (y/n): ")
    if confirm.lower() != 'y':
        exit()
```

### 2. Sá»­ dá»¥ng cache hiá»‡u quáº£
```python
# Dataset Ä‘Æ°á»£c cache tá»± Ä‘á»™ng táº¡i ~/.cache/huggingface/datasets
# Äá»ƒ clear cache: rm -rf ~/.cache/huggingface/datasets
```

### 3. Working vá»›i dá»¯ liá»‡u lá»›n
```python
# Sá»­ dá»¥ng streaming cho datasets khá»•ng lá»“
streaming_dataset = load_dataset("dataset_name", streaming=True)
for sample in streaming_dataset["train"].take(10):
    print(sample)
```

### 4. LÆ°u dataset Ä‘Ã£ xá»­ lÃ½
```python
# LÆ°u processed dataset
processed_dataset.save_to_disk("./my_processed_dataset")

# Load láº¡i
reloaded_dataset = load_from_disk("./my_processed_dataset")
```

## ğŸ¯ Lá»£i Ã­ch chÃ­nh cá»§a Hugging Face Datasets

### âœ… Accessibility & Sharing
- ğŸŒ **Dá»… truy cáº­p**: Chá»‰ cáº§n dataset path
- ğŸ¤ **Dá»… chia sáº»**: TÃ­ch há»£p vá»›i Hugging Face ecosystem
- ğŸ“– **Standardized**: Format thá»‘ng nháº¥t cho táº¥t cáº£ datasets

### âœ… Community Curation
- ğŸ‘¥ **Quality control**: ÄÆ°á»£c cá»™ng Ä‘á»“ng review
- ğŸ¯ **ML-ready**: Sáºµn sÃ ng cho cÃ¡c tÃ¡c vá»¥ ML phá»• biáº¿n
- ğŸ“ **Well-documented**: CÃ³ metadata vÃ  examples Ä‘áº§y Ä‘á»§

### âœ… Performance
- âš¡ **Apache Arrow**: Xá»­ lÃ½ nhanh vÃ  hiá»‡u quáº£
- ğŸ’¾ **Memory optimization**: Sá»­ dá»¥ng RAM tá»‘i Æ°u
- ğŸ” **Fast querying**: Truy váº¥n dá»¯ liá»‡u nhanh chÃ³ng

## ğŸ“‹ Workflow Chuáº©n

1. **ğŸ” Research**: TÃ¬m dataset phÃ¹ há»£p trÃªn Hub
2. **ğŸ“Š Inspect**: Check metadata vÃ  size
3. **ğŸ“¥ Download**: Load dataset vá»›i split phÃ¹ há»£p
4. **ğŸ‘€ Explore**: Xem sample data vÃ  structure
5. **ğŸ› ï¸ Process**: Filter, map, transform theo nhu cáº§u
6. **ğŸ’¾ Save**: LÆ°u processed data náº¿u cáº§n
7. **ğŸš€ Use**: Integrate vÃ o ML pipeline

## ğŸ”— TÃ i liá»‡u tham kháº£o

- [Datasets Documentation](https://huggingface.co/docs/datasets/index)
- [Loading Datasets Guide](https://huggingface.co/docs/datasets/loading)
- [Processing Data](https://huggingface.co/docs/datasets/process)
- [Apache Arrow Overview](https://arrow.apache.org/overview/)

## ğŸ’¡ Tips cho Developer Viá»‡t Nam

- ğŸŒ **Káº¿t ná»‘i internet**: Äáº£m báº£o stable khi download datasets lá»›n
- ğŸ—‚ï¸ **Storage**: Kiá»ƒm tra dung lÆ°á»£ng á»• cá»©ng trÆ°á»›c khi táº£i
- â° **Time management**: Datasets lá»›n cÃ³ thá»ƒ máº¥t nhiá»u thá»i gian download
- ğŸ”„ **Caching**: Táº­n dá»¥ng cache Ä‘á»ƒ trÃ¡nh download láº¡i
- ğŸ“Š **Sample first**: LuÃ´n test vá»›i subset nhá» trÆ°á»›c khi xá»­ lÃ½ toÃ n bá»™
